\mainpage Parallel processing

This document provides a general overview of \c oomph-lib's 
parallel processing capabilities. 

\c oomph-lib is designed so that,
provided the library is compiled with MPI support, many of the
most computationally expensive phases of a typical computation are 
automatically performed in parallel without requiring any user
intervention. Examples of "automatically parallelised" tasks include
- The assembly of the Jacobian matrix and the residual vector in
  Newton's method.
- Error estimation.
- The solution of the linear systems within the Newton iteration, and any
  preconditioning operations performed within \c oomph-lib's block 
  preconditioning framework.
.
Other parallel tasks, such as the distribution of a problem by 
domain decomposition, require minimal user intervention. For instance, 
a call to  \c Problem::distribute() suffices to distribute an \c oomph-lib 
\c Problem object over multiple processors so that each processor
only stores a subset of the elements. Many of \c oomph-lib's
multi-physics helper functions (e.g. the automatic setup of the
fluid load on solid elements in FSI problems; the determination
of "source elements" in multi-field problems; etc) automatically deal
with distributed problems. However, the user can (and for more
complicated problems occasionally has to) intervene in (or be aware
of certain features of) the distribution process. This document 
therefore provides an overview of the underlying design and 
provides pointers to relevant demo driver codes and the associated
tutorials in which some of the more advanced features are discussed.

Here is an overview of the document:

- \ref basics \n\n
  - \ref installation \n\n
  - \ref running \n\n
  - \ref solvers \n\n
  - \ref self_tests \n\n
- \ref domain_decomposition \n\n
  - \ref how_to_distribute
     \n\n
  - \ref how_it_works  
    \n\n
  - \ref advanced
    \n\n
    - \ref pruning
      \n\n
    - \ref face_elements
      \n\n
    - \ref multi_domain
      \n\n
    - \ref alg_node_update
      \n\n
    .
  .
- \ref examples \n\n
  - \ref adaptive_driven_cavity \n\n
  - \ref two_d_poisson_flux_bc_adapt \n\n
  - \ref refineable_b_convection \n\n
  - \ref leaflet \n\n
  - \ref turek \n\n
  .
- \ref problems \n\n
.

#########################################################################
\n\n
<HR>
<HR>
#########################################################################

\section basics Basics

\subsection installation How to build/install oomph-lib with MPI support

- To compile \c oomph-lib with MPI support, specify the configure flag
  \n\n
  \code
  --enable-MPI
  \endcode
  \n
  If you use \c oomph-lib's <code>autogen.sh</code> script to build
  the library you should add this line to the
  <code>config/configure_options/current</code> file.
  You should also ensure that appropriate parallel compilers are specified by 
  the \c CXX, \c CC, \c F77 and \c LD flags. For instance, if you use 
  <a href="http://www.lam-mpi.org/">LAM</a>, 
  you should use \c CXX=mpic++, \c CC=mpicc, \c F77=mpif77 and \c LD=mpif77.
  \n\n
- When \c oomph-lib is built with MPI support, the macro \c
  OOMPH_HAS_MPI is defined. It is (and in newly-written functions should
  be) used to isolate parallel sections of code to ensure that the
  library can be used in serial and parallel. Here is a
  representative code fragment that illustrates the idea:
  \n\n
  \code

  [...]

  #ifdef OOMPH_HAS_MPI

     std::cout << "This code has been compiled with mpi support \n " 
               << "and is running on " << Communicator_pt->nproc() 
               << " processors. " << std::endl;

  #else

     std::cout << "This code has been compiled without mpi support" 
               << std::endl;

  #endif

  [...]

  \endcode
  \n
.

########################################################################
<HR>

\subsection running How to run a driver code in parallel

- As discussed above, many of \c oomph-lib's main tasks are 
  fully parallelised, so, provided you initialise MPI by adding the
  commands \c MPI_Helpers::init(...) and \c  MPI_Helpers::finalize()
  to the beginning and end of the \c main() function, 
  \n\n
  \code   
  int main(int argc, char **argv)
    { 

      // Initialise oomph-lib's MPI       
      MPI_Helpers::init(argc,argv);     
    

      // Normal "serial" code
      [...]  
 
      
      // Shut down oomph-lib's MPI
      MPI_Helpers::finalize();
    }
  \endcode
  \n
  running the code on multiple processors should immediately lead to a speedup.
  How much speedup you can expect depends on the specific
  problem. In most applications, the most computationally expensive tasks
  are the setup of the Jacobian matrix and the solution of the 
  linear systems. In our experience, the assembly of the Jacobian
  matrix tends to scale very well with the number of processors. 
  The parallel performance of 
  the (third-party) linear solvers available from within \c oomph-lib
  varies greatly and their performance is also strongly dependent on
  the underlying hardware, e.g. the speed of your machine's 
  parallel interconnects, etc.
  \n\n
- Note that the general MPI header file is included in \c oomph-lib's
  generic library, so it is not necessary to include it in the
  driver code. Furthermore, the functions \c MPI_Helpers::init(...) 
  and \c  MPI_Helpers::finalize() call their MPI counterparts,
  \c MPI_Init(...) and \c MPI_Finalize(), so they \b must \b not be called
  again. 
  \n\n
- How to actually start a parallel job depends on your particular
  MPI installation. If you use <a href="http://www.lam-mpi.org/">LAM</a>, 
  the executable \c parallel_executable is run on four processors by 
  issuing the command
  \n\n
  \code
  mpirun -np 4 ./parallel_executable
  \endcode
  \n
.


########################################################################
<HR>

\subsection solvers oomph-lib's parallel linear solvers

- \c oomph-lib's default linear solver is 
  <A HREF="http://crd.lbl.gov/~xiaoye/SuperLU">SuperLU</A>. 
  If \c oomph-lib is built with MPI support and the executable is
  run on multiple processors, \c SuperLU's parallel counterpart, 
  \c SuperLU_dist, is used instead. 
  \n\n
- Of \c oomph-lib's own iterative linear solvers, only ***hierher RICHARD
  to fill in details ***
  is parallelised. We recommend using \c oomph-lib's  wrappers to the 
  parallel Krylov subspace solvers from the 
  <a href="https://computation.llnl.gov/casc/linear_solvers/sls_hypre.html">
  Hypre</a> and <a href="http://trilinos.sandia.gov/">
  Trilinos </a> libraries (see 
  <a href="../../../the_distribution/html/index.html#external_dist"> 
  the \c oomph-lib installation page</a> for details on how to install
  these) instead. The interfaces are identical to those
  used to call these solvers in serial; see 
  <a href="../../../linear_solvers/html/index.html"> the linear solver
  tutorial</a> for details.
  \n\n
- \c oomph-lib's block preconditioning framework is fully 
  parallelised ***hierher RICHARD: TRUE? *** and can be used in the 
  same way as in a serial code. ***hierher RICHARD:
  CAN/SHOULD WE SAY SOMETHING MORE ABOUT WHICH OF OUR PRECONDITIONERS ARE
  PARALLELISED? ***
.

########################################################################
<HR>

\subsection self_tests How to include parallel demo codes into the self-tests

- The configure flag \c --with-mpi-self-tests includes 
  \c oomph-lib's parallel demo driver codes into the self-tests 
  that are executed when \c make \c check is run. The self-tests
  assume that the executable is run on two processors and the
  configure flag therefore requires the specification of the command 
  that spawns a two-processor parallel job on the target
  machine. So, under <a href="http://www.lam-mpi.org/">LAM</a>,
  you should provide the configure flag
  \n\n
  \code
  --with-mpi-self-tests="mpirun -np 2"
  \endcode
  \n
  This is most easily done by adding this line to  the
  <code>config/configure_options/current</code> file before
  building/installing \c oomph-lib with <code>autogen.sh</code>.
  \n\n
  \b NOTE: Make sure your MPI demons are started before running the 
  self-tests, e.g. by using the \c lamboot command, otherwise
  the parallel self-tests will fail. 
.

########################################################################
########################################################################

<HR>
<HR>


\section domain_decomposition Distribution of problems by domain decomposition

The parallel capabilities described so far require little, if any, 
user intervention. The parallel speedup is achieved by distributing
the computational work over multiple processors. For instance, when 
assembling the global Jacobian matrix, each processor only assembles the 
contributions from a sub-set of elements. \n\n By default each processor 
stores the entire \c Problem object. This is convenient because it
means that all data is available on all processors but it does, of 
course, limit the size of problem that can be solved. Furthermore, 
the mesh adaptation does not benefit from parallel processing as each processor 
has to adapt its own copy of the entire mesh, even though it only operates
on a subset of the elements when assembling the Jacobian matrix, say.

\c oomph-lib's domain decomposition procedures allow a \c Problem
to be distributed over multiple processors in such a way that
each processor only holds a fraction of the \c Problem's elements. 
This can lead to substantial reductions in memory usage and
allows the mesh adaptation to be performed in parallel. 

########################################################################

<HR>

\subsection how_to_distribute Basic usage: How to distribute a simple problem

- In most cases the problem distribution is extremely
  straightforward. Given an existing serial driver code
  (modified by the addition of calls to \c MPI_Helpers::init(...) 
  and \c  MPI_Helpers::finalize(), as discussed above) the function
  \n\n
  \code
  Problem::distribute()
  \endcode
  \n
  can be called at any point \b after the \c Problem has been 
  constructed (i.e. at a point at which \c Problem::newton_solve()
  could be called), but \b before any <b>non</b>-uniform mesh refinement has 
  taken place. *** hierher: Andy do we check this? ****
  \n\n
  Following the call to \c Problem::distribute() each processor
  only holds a sub-set of the \c Problem's elements: the elements
  the processor is "in charge" of, and additional halo elements
  that are retained to facilitate the subsequent mesh adaptation.
  (Halo elements are discussed in more detail below). We note that
  even though the processors no longer store all elements, the
  meshes' boundary lookup schemes are updated during the
  distribution process so that \c Mesh::nboundary_element(b) returns the
  number of elements that are \b now adjacent to boundary \c b, etc.
  This means that functions that were written (in serial) to update
  time-dependent boundary conditions on a mesh's boundary nodes, say, continue 
  to work in parallel without requiring any modifications. 
  \n\n
- Since each processor only holds a subset of the \c Problem's 
  elements, it is sensible to modify the post-processing 
  routines such that output files are labeled by the processor 
  number. 
  \n\n
  *** hierher: Andy write something more transparent (along the
  lines shown below that also uses the Problem's communicator
  \n\n
  \code

  [...]

  // Get number of processor from the problem's communicator
  int my_rank=Problem::Communicator_pt->nproc();

  // Label filename by processor number
  char filename[100];
  sprintf(filename,"solution_on_processor_%i.dat",my_rank);

  // Output
  ofstream some_file;
  some_file.open(filename);
  mesh_pt()->output(some_file);
  some_file.close();

  [...]

  \endcode
  \n
  Note that, by default, \c oomph-lib's mesh-based output functions include
  output from halo-elements ***hierher: is this true/what we want? ***
  The output from halo-elements can be suppressed/re-enabled 
  by calls to 
  \n\n
  \code
  Mesh::suppress_halo_element_output()
  \endcode
  \n 
  and
  \n\n 
  \code
  Mesh::enable_halo_element_output()
  \endcode
  \n
  respectively.
  *** hierher: implement ***
  \n\n
- Other, equally straightforward modifications to existing driver codes
  tend to be required if the serial version of the code contains explicit
  references to individual elements or nodes, e.g. in order to pin a pressure
  value in a Navier-Stokes computation with enclosed boundaries. 
  In such cases, it is important to remember that, once the problem 
  is distributed, \b (i) not every processor has direct access to a
  specific element (or node), and  \b (ii) the pointer to the "first" 
  element in a mesh (say) points to a different element on each processor.
  This will become clearer once we discuss details of the
  implementation in the next section; the particular example 
  is discussed in more detail in the section discussing the
  distributed solution of the \ref
  adaptive_driven_cavity .
. 

########################################################################
<HR>

\subsection how_it_works Overview of the implementation

  The main task of the \c Problem::distribute() function is
to distribute the \c Problem's global mesh (which possibly
comprises multiple sub-meshes) amongst the processors so that 
<b>(a)</b> the storage requirements on each processor are reduced;
<b>(b)</b>  during the mesh adaptation each processor only acts
on a fraction of the overall mesh; while ensuring that <b>(c)</b> 
communication between processors to synchronise the data structures that are
shared between multiple processors is minimised. The key challenge
for the implementation is to retain (as much as possible) the 
efficient existing mesh adaptation procedures which employ 
generalisations of pointer-based quad/octree data structures 
(\c TreeForests -- oriented collections of \c Tree
objects which allow efficient storage and manipulation of 
the refinement patterns in block-structured meshes) 
to determine neighbouring elements and to identify
hanging nodes. Since pointer-based data structures cannot be 
communicated directly between different processors, the relevant 
data structures are serialised to enable their synchronisation 
via standard MPI communications routines.



The figure below shows a conceptual sketch of the parallelisation
strategy adopted. For simplicity, we shall restrict the discussion
to the 2D case and ignore various complications that arise with more 
complicated mesh partitionings.

@I w 0.75\textwidth decomposition "Sketch illustrating the phases of the  parallel mesh adaptation procedure for a problem that is distributed over four processors. The columns illustrate the evolution of the mesh on each of the four processors. The objects' colours indicate which processor is 'in charge' of them. "



The problem distribution and parallel mesh adaptation 
proceeds as follows: 
 

- <b>Stage 1: Initial refinement and partitioning (serial)</b>
  \n\n
  First each processor constructs the \c Problem object, 
  using a (typically very coarse) initial mesh -- in the figure above,
  a mesh containing a single four-node quad. Repeated
  calls to  \c Problem::refine_uniformly() are then made to increase the
  number of elements sufficiently for a sensible
  mesh partitioning (with \c METIS) to be possible.
  Based on the output from \c METIS, each element is associated
  with a unique processor. Nodes that are located in the interior of a
  processor's patch of elements are associated with that processor;
  nodes that are shared by elements that are associated
  with different processors are associated with the highest-numbered
  one. ***hierher highest or lowest? Check and update sketch if
  necessary***
  \n\n
- <b>Stage 2: Identification of halo[ed] nodes/elements and pruning 
  (parallel)</b>
  \n\n
  Each processor now decides which elements and nodes it has to retain. 
  In the first instance, each processor retains its own elements and
  nodes -- they are the objects it is ``in charge of''. In addition,
  each processor retains a single layer of elements adjacent to 
  its own elements, as well as their nodes. We shall refer
  to retained elements/nodes that a processor is not in charge of
  as ``halo'' elements/nodes. Similarly, we refer to objects as ``haloed''
  if the processor is in charge of them but they have halo counterparts
  on other processors. 
  \n\n
  At this stage of the process, each processor still 
  has access to the entire mesh. It is therefore possible to establish 
  a consistent numbering scheme for halo[ed] elements/nodes, so that, 
  e.g. \c Mesh::halo_node_pt(j,p), executed on processor \c q,
  returns a pointer to processor \c q 's \c j-th halo node whose non-halo 
  counterpart is located on processor \c p. The corresponding
  non-halo node may be identified by a call to \c Mesh::haloed_node_pt(j,q),
  executed on processor \c p. Similar lookup schemes are generated
  for the halo[ed] elements which also act as roots for the 
  \c Trees that will record their possible subsequent refinement. The 
  access function  \c Mesh::root_halo_element_pt(j,p), executed on 
  processor \c q, therefore provides ordered access to all 
  halo elements whose non-halo counterparts are located on processor
  \c p, etc.
  \n\n
  Once this information has been set up, the superfluous nodes and elements
  are deleted and the mesh's boundary lookup-schemes (required to identify the
  nodes and elements that are located next to domain boundaries) and
  its \c TreeForest representation are re-generated. [It is possible
  to retain all elements in a mesh as halo elements. This is useful
  in certain free-boundary problems; see the section 
  \ref alg_node_update below for details]. Finally, the equation 
  numbers are re-assigned in a two-stage process. First, each processor 
  independently assigns the equation numbers for the unknowns associated 
  with the nodes it is in charge of. MPI-based communication routines are then
  used to synchronise the equation numbers between the processors.
  \n\n
  All the functionality described above is implemented in a single
  function, \c Problem::distribute(). Following its execution
  on all processors, each processor can assemble its contribution to the 
  distributed Jacobian matrix and residual vectors, required by
  \c oomph-lib's parallel linear solvers, by referring only 
  to locally stored non-halo objects. Once the Newton correction 
  to the unknowns has been computed, each processor updates the unknowns 
  associated with the nodes it is in charge of, before MPI-based 
  communication is employed to update the unknowns stored at 
  the processors' halo nodes.
  \n\n
- <b>Stage 3: Parallel mesh adaptation (parallel)</b>
  \n\n
  Once a problem is distributed, further mesh refinement can 
  be performed in parallel by using the already existing mesh adaptation 
  procedures on the (partial) meshes that are held on the various 
  processors. For spatially non-uniform refinement 
  the decision of whether or not to refine an element is taken by 
  the haloed element and communicated to its halo counterparts before
  the adaptation is performed.
  Following the refinement, each newly-created node is associated 
  with a unique processor, using the rules described above, and halo[ed] 
  nodes are identified and added to the appropriate lookup schemes. 
  Ordered access to the newly-created halo[ed] elements is obtained 
  by traversing the \c Trees associated with the root halo[ed] 
  elements that were identified in stage 2.
  For a distributed \c Problem, these steps are performed automatically
  when \c Problem::refine_uniformly() or any of the other
  mesh adaptation routines within \c oomph-lib are executed.
  \n\n
- <b> Stage 4: Optional pruning of superfluous halo[ed] nodes and 
  elements (parallel) </b>
  \n\n
  The parallel efficiency of the distributed mesh adaptation
  (in terms of the memory required to hold the partial meshes, and 
  in terms of the CPU time required for their adaptation) is 
  limited by the fact that each processor has to adapt not only 
  the \f$N_{\rm in \ charge}\f$ elements it is in charge of, 
  but also its \f$N_{\rm halo}\f$ halo elements. We define the efficiency 
  of the problem distribution as
  \f[
  e_{dist} = 
  \frac{N_{\rm in \ charge}}{N_{\rm halo} + N_{\rm in \ charge}} 
  \le 1,
  \f]
  where the equality could only be achieved in the absence of any 
  halo elements. The efficiency is determined during the initial problem 
  distribution and at that stage of the process it
  can only be improved by <b>(i)</b> increasing the number of non-distributed
  initial mesh refinements; <b>(ii)</b> reducing the number of
  processors. Since both options reduce the parallelism they are not
  desirable. It is possible, however, to improve the parallel 
  efficiency by  pruning superfluous halo[ed] elements after each
  mesh refinement, as illustrated in the figure above. If this is done 
  after every  mesh adaptation, \f$e_{dist}\f$ increases significantly as 
  the refinement proceeds. However, the pruning of halo[ed] nodes and elements 
  makes the refinement irreversible since it alters the
  \c TreeForest structure of the mesh: once the superfluous
  objects have been removed, the mesh's \c TreeForest representation
  must be re-generated and the mesh can then no longer be unrefined. 
  \n\n
.


########################################################################

<HR>
<HR>

\subsection advanced More advanced features 

The following features are more advanced in the sense that they
require some knowledge of the way in which \c Problems are
distributed within \c oomph-lib. Assuming you have read through 
the previous section, you should be able to appreciate the
slight subtleties involved... 


<HR>

\subsubsection pruning Pruning halo elements and nodes
When a problem is distributed by \c Problem::distribute() 
each processor retains a halo layer of elements that surround
the (non-halo) elements it is "in charge" of. When the mesh is first
distributed, the halo layer has a depth of one element. Its presence 
facilitates the synchronisation of any subsequent mesh refinement,
but also reduces the parallel efficiency of the code. 
Repeated mesh refinement can make the halo layers (which comprise
the original halo elements and their sons) much thicker than
a single element layer. The superfluous halo elements can be removed
by calling the function
\n\n
\code
Problem::prune_halo_elements_and_nodes()
\endcode
\n
Note that, following a call to this function, the mesh(es) involved
can no longer be unrefined below the previous highest level of uniform
refinement.



########################################################################
<HR>

\subsubsection face_elements Distributing problems involving FaceElements

\c FaceElements are typically used to apply Neumann/traction-type
boundary conditions. See, e.g., the tutorials that discuss the
application of such boundary conditions in
<a href="../../../poisson/two_d_poisson_flux_bc/html/index.html">
Poisson</a> or
<a href="../../../navier_stokes/rayleigh_traction_channel/html/index.html">
Navier-Stokes</a> equations. Since the \c FaceElements
that apply the Neumann boundary conditions are attached
to "bulk" elements that may disappear during mesh adaptation, 
we generally recommend to store the (pointers to the) 
\c FaceElements in a separate mesh, and to use
the \c Problem::actions_before_adapt() and 
\c Problem::actions_after_adapt() functions to detach and re-attach 
the \c FaceElements to/from the bulk elements before and after the
mesh adaptation.

  The same issues arise during the problem distribution: A 
\c FaceElement that was created before the problem was distributed
may have been attached to a bulk element that is deleted when the
distribution is performed, resulting in obvious (and disastrous) 
consequences. We therefore recommend using the functions
\n\n
\code
Problem::actions_before_distribute()
\endcode 
\n
and
\n\n
\code
Problem::actions_after_distribute()
\endcode 
\n
to detach and re-attach any \c FaceElements before and after the
problem distribution. In this context it is important to note that:
\n
-# The \c FaceElements \b should be available before \c Problem::distribute() 
   is called to allow the load balancing routines to take their 
   presence into account. 
   \n\n
-# \c FaceElements that are attached to halo (bulk-)elements become
   halo-elements themselves. *** hierher Andy: How are they
   accessed?***
.
Further details are provided in  
<a href="../../../hierher***andy_to_write_this">another 
tutorial</a> which explains the modifications
to the serial driver code required to distribute
a Poisson problem with Neumann boundary conditions.


########################################################################
<HR>

\subsubsection multi_domain Distributing multi-domain problems

Multi-domain problems are problems that involve interactions
between PDEs that are defined in different domains, such as
fluid-structure interaction problems. We note that, as discussed
<a href="***hierher***">elsewhere</a>, it is sometimes useful to treat
multi-field problems (i.e. problems involving interactions 
between PDEs that are defined in the \b same domain, e.g. Boussinesq
convection) as multi-domain problems, because this allows the
use of separate error estimators and refinement patterns
for the different fields. 

 Within \c oomph-lib, multi-domain problems typically involve
elements that are derived from the \c ElementWithExternalElement
base class. Such elements store a pointer to the "external" finite
elements (and local coordinates within these) that provides
the source (or traction) term for its equations. For instance,
in an FSI problem, the \c FSIWallElement is derived from the \c
ElementWithExternalElement class; its "external" elements are the 
adjacent fluid elements that provide the fluid traction at the 
\c FSIWallElement's Gauss points. "External" elements tend to be
determined by helper functions such as 
 \c FSI_functions::setup_fluid_load_info_for_solid_elements() or
\c Multi_domain_functions::set_sources(...), both of which use the
function \c Multi_domain_functions::set_external_storage(...) ***should this
be called something else? Maybe locate_external_elements() ? *** 
to determine the "external" elements. 

The implementation of multi-physics interactions is greatly
facilitated if "external" elements are available on the same 
processor as the \c ElementWithExternalElements for which
they provide the source/traction term. The helper functions referred
to above therefore work in two stages when locating the
"external" elements:
-# They start by looping over all \c ElementWithExternalElements 
   *** incl halo??? *** and try to locate
   the "external" elements (e.g. the fluid elements that are adjacent
   to the elastic wall in an FSI problem) on their own processor. 
   When the required "external" element is found locally, the 
   \c ElementWithExternalElement stores a pointer to it.
   \n\n
-# If the "external" element cannot be found locally, MPI-based
   communication is employed to find the "external" element on one
   of the other processors. Once found, a halo-copy of the "external"
   element (and its nodes) is made on the current processor and a
   pointer to the halo-element is stored. The "external" halo elements 
   and nodes are stored in the mesh
   they are associated with, so that, in an FSI problem, the
   "external" fluid elements are added to the fluid mesh. 
   \n\n
.

Access to "external" halo elements is provided via \c Mesh member 
functions such as
\n\n
\code
Mesh::nexternal_halo_element()
\endcode
\n
and
\n\n 
\code
Mesh::nexternal_haloed_element()
\endcode
\n
which return the number of "external" halo[ed] elements stored in a 
mesh, and
\n\n
\code
Mesh::external_halo_element_pt(...)
\endcode
\n
and 
\code
Mesh::external_haloed_element_pt(...)
\endcode
\n
which return pointers to the "external" halo[ed] elements in the
mesh, etc.
\n\n
"External" halo[ed] elements are automatically included in any
halo/haloed synchronisation operations performed when assigning
equation numbers, or updating unknowns during the Newton iteration,
etc. 


The procedure discussed above has two important consequences:
\n
- The fact, that calling 
  \c FSI_functions::setup_fluid_load_info_for_solid_elements() or
  \c Multi_domain_functions::set_sources(...) sets pointers to 
  "external" elements implies that such functions must be called
  again after the problem has been distributed, e.g. by
  invoking them again in \c Problems::actions_after_distribute().
  This is because some of the "external" elements initially pointed to by an \c
  ElementWithExternalElements may no longer exist after the
  problem has been distributed. Such "external" elements must be re-generated
  as "external" halo elements.
  \n\n
- When an "external" halo element is created we also automatically create
  halo-copies of those of its nodes that do not already exist
  on the current processor. Such nodes are stored as "external" halo nodes 
  and they are automatically synchronised with their non-halo
  counterparts on other processors. However, synchronisation of nodes
  does not (and cannot) include the specification of auxiliary node 
  update functions (such as the function 
  \c FSI_functions::apply_no_slip_on_moving_wall(...) which automatically 
  applies the no-slip condition on moving fluid-solid interfaces). 
  Such functions should therefore be re-assigned to the appropriate nodes
  after \c FSI_functions::setup_fluid_load_info_for_solid_elements() 
  has been called. This is exactly equivalent to the sequence of steps
  required following an adaptive mesh refinement; see e.g. the
  <a href=
  "../../../interaction/fsi_collapsible_channel_adapt/html/index.html#before_and_after">tutorial
  discussing the adaptive solution of the collapsible channel problem</a>
  for a more detailed discussion of this issue. We note that "external"
  halo-nodes are added to the mesh's boundary lookup schemes ***hierher
  is this true?? ***, so the
  specification of auxiliary node update functions for all nodes
  on a given mesh boundary does not require any further modification
  to the serial code.  
.



########################################################################
<HR>

\subsubsection alg_node_update Distributing problems involving meshes with algebraic node updates

\c oomph-lib provides a variety of algebraic node-update methods. 
These allow the fast and sparse update of the nodal positions 
in response to changes in the domain boundaries. The shape and 
position of such boundaries is typically represented by one or 
more \c GeomObjects. If the motion of the boundary is prescribed, 
(as in the case of the
<a href="../../../navier_stokes/osc_ellipse/html/index.html">flow 
inside an oscillating ellipse</a>, say) no modifications are
required when the meshes are used in a distributed problem.

In many free-boundary problems the position of the domain boundary 
has to be determined as part of the solution. For instance, in 
a fluid-structure interaction problem, the \c GeomObject that
defines the interface between the fluid and solid domains is 
typically a \c MeshAsGeomObject -- a compound \c GeomObject that is
formed from the mesh of \c FSIWallElements that bound the fluid domain. 
In such problems, the algebraic node update operations tend to introduce 
"far away" interactions between elements of different type. For 
instance, in the 
<a href=
"../../../interaction/fsi_collapsible_channel_algebraic/html/index.html">
collapsible channel problem</a> the positional degrees of freedom 
of the \c SolidNodes in the wall mesh affect the nodal positions 
of the fluid elements "underneath" these \c SolidNodes. 

To ensure that algebraic node-update operations that were implemented
in a serial context continue to work when the problem is distributed,
the halo elements in the wall mesh from which the \c MeshAsGeomObject
is built should \b all be retained when the mesh is distributed. 
This leads to a slight increase in the overall storage requirements 
(because none of the \c FSIWallElements are deleted when the wall mesh
is distributed) but it means that the entire \c GeomObject remains
accessible to the fluid mesh without invoking MPI communications. 

 ***hierher Andy to fill in how/where the retention is controlled ***


We stress that the increase in storage requirements due to the
retention of all \c FSIWallElements is minimal because the  
\c FSIWallElements are only located along the (low-dimensional) boundaries 
of the fluid mesh. For instance, in the <a href=
"../../../interaction/fsi_collapsible_channel_algebraic/html/index.html">
collapsible channel problem</a> the 1D mesh of beam elements
bounds the 2D mesh of fluid elements; in
<A HREF="../../../interaction/turek_flag/html/index.html">Turek and 
Hron's FSI benchmark problem</a>, the 2D fluid domain is bounded
by a 1D mesh of \c FSISolidTractionElements; etc.





########################################################################
########################################################################

<HR>
<HR>

\section examples Overview of representative parallel driver codes

Several separate tutorials are provided to illustrate the steps 
required to modify existing serial driver codes so that 
they works in parallel.  In general, all parallel drivers codes 
have to:

- call \c MPI_Helpers::init(...) and \c MPI_Helpers::finalize()
  at the beginning and end of \c main(),
  \n\n
- modify the names of the output files to distinguish output 
  from different processors, and
  \n\n
- call \c Problem::distribute() at some point after the problem has
  been constructed, but before any non-uniform refinement has taken 
  place.
.

########################################################################
<hr>

\subsection adaptive_driven_cavity Adaptive driven cavity problem
The serial driver code for the
<a href="../../../navier_stokes/adaptive_driven_cavity/html/index.html">
adaptive driven cavity problem</a> only requires one additional 
modification. Since the flow is enclosed, one pressure value
has to be pinned. In the serial driver code, we arbitrarily pin
the first pressure degree of freedom in the first element -- in the
distributed version of the problem this requires some care since,
following the problem distribution the "first" element is a different
one on each processor. We discuss the required changes to the driver
code in a <a href="../../../***hierher/html/index.html">
separate tutorial</a>.


########################################################################
<HR>

\subsection two_d_poisson_flux_bc_adapt 2D Poisson problem with flux boundary conditions

The only additional modification to the serial driver code for the
<a href="../../../poisson/two_d_poisson_flux_bc2/html/index.html">
Poisson problem with flux boundary conditions</a> 
is the provision of the \c Problem::actions_before_distribute() and
\c Problem::actions_after_distribute() functions to detach and
re-attach the \c FaceElements that apply the flux boundary condition
before and after the problem distribution.
This is discussed in a <a href="../../../***hierher/html/index.html">
separate tutorial</a>.


########################################################################
<HR>

\subsection refineable_b_convection Adaptive Boussinesq convection problem

The distribution of the multi-domain version of the
<a href="../../../***hierher/html/index.html">
Boussinesq convection problem</a> is straightforward. It only
requires the re-assignment of the "external" elements after the
problem distribution. Details are explained in 
<a href="../../../***hierher/html/index.html">yet another tutorial</a>.

########################################################################
<HR>

\subsection leaflet Flow past an elastic leaflet

The problem of the
<a href="../../../interaction/fsi_leaflet/html/index.html">
flow past an elastic leaflet</a> requires a few straightforward 
modifications to the serial code:
- We have to retain the elements in the wall mesh to ensure that
  that algebraic node update continues to work properly.
  \n\n
- Following the problem distribution, we have to (re-)set up the
  fluid structure interaction by calling \c 
  \c FSI_functions::setup_fluid_load_info_for_solid_elements() 
  in \c Problem::actions_after_distribute(). 
  \n\n
- We have to re-assign auxiliary node update function to all
  fluid nodes on the FSI boundary in \c
  Problem::actions_after_distribute(). 
  \n\n
. 
The relevant details are discussed in a 
<a href="../../../***hierher/html/index.html">separate tutorial</a>.


########################################################################
<HR>

\subsection turek Turek and Hron's FSI benchmark problem

There are a few further complications involved if the solid and fluid
meshes in an FSI problem are of the same dimension, or if 
the solid mesh is itself refineable; as an example, we revisit 
<A HREF="../../../interaction/turek_flag/html/index.html">Turek 
and Hron's FSI benchmark problem</a>.  
The <a href="../../../hierher/html/index.html">tutorial 
that discusses the distribution of this problem</a>
also explains a few additional modifications that are required
to allow both the fluid and solid meshes to be adapted. 



########################################################################
########################################################################

<HR>
<HR>

\section problems Trouble-shooting and debugging

\subsection checking_and_documenting Debugging and documenting the distribution

Once a problem has been distributed, the function
\code
Problem::check_halo_schemes()
\endcode
can be called to check that the halo lookup schemes for each mesh 
are setup correctly.

Details about the mesh distribution can be generated by
calling
\code
Mesh::doc_mesh_distribution(DocInfo& doc_info)
\endcode
which outputs the elements, nodes, halo(ed) elements, halo(ed) nodes, mesh, 
boundary elements and boundary nodes on each processor.  This routine
is automatically called when \c Problem::distribute() is called with a
\c DocInfo object whose \c doc_flag() is set to true.


########################################################################

<hr>

\subsection parallel_debug Debugging parallel code

Parallel code can obviously fail in many more ways than a code
that runs on a single processor. Here is a procedure that 
allows basic parallel debugging without requiring access to 
expensive commercial tools such as totalview, say.
(The instructions below assume that you use 
<a href="http://www.lam-mpi.org/">LAM</a> as your MPI installation; 
they can probably be modified to work with other versions of MPI, too). 

Let's assume you use <a href="http://www.gnu.org/software/gdb/">gdb</a> 
as your debugger. To debug a serial code
with <a href="http://www.gnu.org/software/gdb/">gdb</a> you would 
load the executable <code>a.out</code> into the debugger using
the command
\code
gdb ./a.out
\endcode
on the command line. Once inside 
<a href="http://www.gnu.org/software/gdb/">gdb</a>, you run 
the code by typing "run". If the code crashes, typing "where" will tell
you in which line of the code the crash occured, and it will also
provide a traceback of the function calls that got you to this
point. 

 To do this in parallel, we have to run each (parallel) instance of
the code within its own <a href="http://www.gnu.org/software/gdb/">gdb</a>
session. To this, create the following three files:
- A shell script \c mpidb that must be executable

** hierher complete ***

@@END@@


Taking the 
<a href="../../../poisson/two_d_poisson_flux_bc_adapt/html/index.html">2D 
Poisson equation with flux boundary conditions</a> as our example this
time, we note that in this case the procedures we need to follow are
identical to those already followed by \c
Problem::actions_before_adapt() and \c Problem::actions_after_adapt()
and consequently, all that is required in addition to distributing the
problem as detailed above is the addition of

\dontinclude two_d_poisson_flux_bc_adapt.cc
\skipline Actions before distribute:
\until }
\skipline Actions after distribute:
\until }

to the problem constructor.

For full sources of this driver code see
<CENTER>
<A HREF="../../../../demo_drivers/mpi/distribution/two_d_poisson_flux_bc_adapt/two_d_poisson_flux_bc_adapt.cc">
demo_drivers/mpi/distribution/two_d_poisson_flux_bc_adapt/two_d_poisson_flux_bc_adapt.cc
</A>
</CENTER>

########################################################################
<HR>

\subsubsection refineable_b_convection Refineable Boussinesq convection problem

An interaction described in the previous section can take place between
two meshes that occupy the same physical space; for example, a
Boussinesq convection problem where there is an "interaction" between
the advection-diffusion equation and the Navier--Stokes equations in
the sense that each equation has a source term which depends upon 
parameters that are obtained from the solution of the other equation.
(The benefit of doing this, compared to combining the elements and
using a single domain, is that a single error estimator can be used
for each individual mesh of elements rather than some combined error 
estimator which has to take into account all of the spatial variables
in a combined element.)

In the Boussinesq example, the "external" elements contain information required
by the source terms in each set of equations:
<center>
\f[ \rho_{0} \left (\frac{\partial u^{*}_i}{\partial t^{*}} +
u^{*}_j \frac{\partial u^{*}_i}{\partial x^{*}_j} \right) =
- \frac{\partial p^{*}}{\partial x^{*}_i} +  \left[\rho_{0} -
  \alpha\rho_{0}(\theta^{*}
 - \theta_{0})\right] G^{*}_i + \mu_{0}
\frac{\partial }{\partial x^{*}_j} \left[
\frac{\partial u^{*}_i}{\partial x^{*}_j} +
\frac{\partial u^{*}_j}{\partial x^{*}_i} \right],
\f]
</center>
is the momentum equation with an additional buoyancy term dependent
upon temperature \f$ \theta^{*} \f$, and
<center>
\f[ \frac{\partial \theta^{*}}{\partial t^{*}} +
u^{*}_{j}\frac{\partial \theta^{*}}{\partial x^{*}_{j}} =
\kappa\frac{\partial}{\partial x^{*}_{j}}\left(\frac{\partial
\theta^{*}}{\partial
x^{*}_{j}}\right),\f]
</center>
is the advection-diffusion equation where the "wind" is given by the
fluid velocity \f$ u^{*}_j \f$.  [See 
<a href="../../../multi_physics/b_convection/html/index.html">the
tutorial on Boussinesq convection</a> for
further detail.]

We therefore take the advection-diffusion and Navier--Stokes elements
required for these sets of equations, and create new elements which
also inherit from the class \c ElementWithExternalElement.  Within
these new elements we overload the functions which provide these
source terms so that they get the required information directly from 
the "external" element.  In this instance, therefore, we overload \c
get_body_force_nst(...) and \c get_wind_adv_diff(...) as follows:

\dontinclude my_boussinesq_elements.h
\skipline start_of_get_body_force_nst
\until end of get_body_force_nst

\skipline start_of_get_wind_adv_diff
\until end of get_wind_adv_diff

For example driver codes for the distributed Boussinesq problem using 
both single- and multi-domain methods, see
<CENTER>
<A HREF="../../../../demo_drivers/mpi/multi_domain/boussinesq_convection">
demo_drivers/mpi/multi_domain/boussinesq_convection
</A>
</CENTER>

The same principle applies to any problem where two sets of
equations "interact" within the same physical space: (1) call the \c
set_sources(...) function to set up the interaction, and
(2) ensure that the functions which provide the relevant source terms
within these equations are overloaded to use the external elements
which were set by the call to \c set_sources(...).

\subsubsection fsi_examples FSI examples

In most of the FSI examples in \c oomph-lib the solid mesh has a lower 
dimension than the fluid mesh.  In these cases the modification
required to make a serial driver code work in parallel is straightforward:
when the solid mesh is turned into a \c MeshAsGeomObject, the user
needs to ensure that all the elements within the solid mesh are kept
available on every processor after a distribution has taken place.
The partition will determine which processor these elements should
initially be kept on; if the \c keep_all_elements_as_halos() flag for
the solid mesh is set to true, then these elements become halo
elements on every other processor involved.  This flag is set by
passing in an optional boolean parameter to the \c MeshAsGeomObject
constructor, e.g.:
\code
MeshAsGeomObject<1,2,FSIHermiteBeamElement>* wall_geom_object_pt= 
 new MeshAsGeomObject<1,2,FSIHermiteBeamElement>(Wall_mesh_pt);
\endcode

[Note: this will only work if the solid mesh used to create the
\c MeshAsGeomObject is not refined in any way; see the \ref turek_flag
example for how to deal with this scenario.]

The \c actions_after_distribute() function, as with the \c
actions_after_adapt() function, must reset anything that could have
been changed by a modification of a mesh or its elements and nodes.
In most cases this is simply achieved by calling \c
actions_after_adapt() from \c actions_after_distribute() as in the
earlier example (see \c actions_routines).

A few further modifications to the order in which functions are called
are also necessary: any auxiliary node update functions must be set
\b after a call to \c setup_fluid_load_info_for_solid_elements() from 
within \c actions_after_adapt() or \c actions_after_distribute().

Examples of modified driver codes for FSI problems for a collapsible
channel, an oscillating ring and a leaflet loaded from both sides by a
fluid can be found in
<CENTER>
<A HREF="../../../../demo_drivers/mpi/multi_domain/">
demo_drivers/mpi/multi_domain
</A>
</CENTER>

########################################################################
<HR>

\subsubsection turek_flag Turek flag problem

There are a few further complications involved if the solid and fluid
mesh are of the same dimension, or if the solid mesh is itself
refineable; as an example, we revisit 
<A HREF="../../../interaction/turek_flag/html/index.html">Turek and Hron's FSI benchmark problem</a>.  
This particular parallel driver has been modified further from the original
serial version to allow the solid mesh to refine as well as the fluid
mesh; we explain here the changes required within the 
<a href="../../../interaction/turek_flag/html/index.html">original
driver code</a> to allow for both refinement of the solid mesh and the
distribution of the whole problem.

Firstly, as with all problems involving meshes built from traction
elements, \c actions_before_distribute() must ensure that such meshes
are removed from the global mesh storage before the distribution takes
place.  We must also ensure that the solid elements used to build
these traction elements are kept on every processor.  This is achieved
by looping over the traction meshes and setting the flag \c
GeneralisedElement::must_be_kept_as_halo() for all their elements to
true, giving the following:

\dontinclude turek_flag.cc
\skipline start_of_actions_before_distribute
\until end of actions before distribute

[Note: the traction meshes have not yet been deleted!  This is
explained in more detail later...]

In \c actions_after_distribute() we need to delete the traction
elements and re-attach them to the newly distributed solid mesh.  This
required an additional function to perform the deleting:

\skipline start_of_delete_traction_elements
\until end of delete traction elements

to complement the already-exisiting \c create_fsi_traction_elements()
function.  The \c actions_after_distribute therefore reads as follows:

\dontinclude turek_flag.cc
\skipline start_of_actions_after_distribute
\until build of FSISolidTractionElements is complete

Now that the traction meshes have been rebuilt, we create new
\c MeshAsGeomObjects from them and tell the (algebraic) fluid mesh:

\until set_tip_flag_pt

As the sub-\c GeomObjects of these \c MeshAsGeomObjects may have changed, we
must now call the \c update_node_update() function again for each
node in the fluid mesh:

\until }

Now we add the traction meshes back to the problem and rebuild the
global mesh:

\until rebuild_global_mesh

Finally, we can now re-set the fluid load on the solid elements for
each of the relevant boundaries within the fluid mesh, and then re-set
the auxiliary node update function:

\until end of actions after distribute

For the complete modified driver code, see
<CENTER>
<A HREF="../../../../demo_drivers/mpi/multi_domain/turek_flag/turek_flag.cc">
demo_drivers/mpi/multi_domain/turek_flag/turek_flag.cc
</A>
</CENTER>

########################################################################
########################################################################

<HR>

\section problems Trouble-shooting

- Debugging?
- doc_mesh_distribution(...)





@@END@@


########################################################################
<HR>

\subsection distribute_options Further user options

The function
\code
Problem::distribute(...)
\endcode

can be called with various arguments:

- No arguments
- A boolean \c report_stats flag which outputs to the screen during a job.
- A \c DocInfo object which allows output of the complete domain of
and the mesh, elements and nodes on each processor
- A vector of unsigned integers detailing the partition to be used
(used during demo runs; this must be the same size as the number of
elements that are to be distributed, ie. it must ignore any elements
that may be removed in \c actions_before_distribute(), say)
- Any combination of the above arguments (with the Vector always first, 
and boolean always last)

The function itself also returns a vector of unsigned integers detailing
the partition that has actually been used during the distribution.

\subsubsection prune_halo_elements Prune halo elements and nodes

A call to the \c distribute() function sets up a halo-haloed lookup 
scheme across boundaries between processors (see \ref
distribute_details for more information); following this, any
subsequent refinement will include these elements, and the element
sons of halo elements automatically become halo themselves.  If a
large amount of refinement takes place after the problem has been
distributed, then layers of halo elements (and nodes) may become
excessively wide. Therefore, another routine available to the user, 
once distribution has taken place, is
\code
Problem::prune_halo_elements_and_nodes()
\endcode
This routine will reduce the number of halo(ed) elements and nodes 
once refinement has taken place on a distributed problem.  It works
by reducing the mesh (on each processor) to its highest uniform
state of refinement and then removing any unnecessary halo elements and 
nodes at that level of refinement, plus any sons of these
elements at any subsequent level of refinement.  In the figure, (a)
shows elements on processor 0 after a call to \c problem.distribute() 
and its haloed elements with processor 1; (b) shows the result of a 
uniform refinement, with all sons of haloed elements becoming haloed 
elements; and (c) shows the result of a call to \c
problem.prune_halo_elements_and_nodes(), which removes the unnecessary
layer of halo elements created as a result of the uniform refinement.
Note that, following a call to this function, the mesh(es) involved
may no longer be unrefined below the previous highest level of uniform
refinement.

@I w 0.75\textwidth prune_halo "Example of pruning halo elements and nodes. "

\subsubsection checking_and_documenting Debugging and documenting the distribution

Once a problem has been distributed, an optional routine available to the
user is
\code
Problem::check_halo_schemes()
\endcode
which ensures that the halo lookup schemes (see \ref distribute_details) 
for each mesh are correct by checking information in halo elements
against the information held in their non-halo counterparts on another 
processor. This is a routine which only needs to be called if the user is
suspicious of any errors occurring along boundaries between processor
domains.  It can be called with a \c DocInfo flag which allows output of 
halo(ed) elements and nodes between each possible pair of processors.

A further output routine is also available for each (sub)mesh:
\code
Mesh::doc_mesh_distribution(DocInfo& doc_info);
\endcode
outputs the elements, nodes, halo(ed) elements, halo(ed) nodes, mesh, 
boundary elements and boundary nodes on each processor.  This routine
is automatically called when \c Problem::distribute() is called with a
\c DocInfo object with \c doc_flag() set to true.

########################################################################
<HR>

\subsection multi_domain Distributing a multi-domain problem

For multiple domains, any interaction that may occur is set by the user
within a driver code, for example, using the function
\code
FSI_functions::setup_fluid_load_for_solid_elements(...)
\endcode
in an FSI problem, or the function
\code
Multi_domain_functions::set_sources(...)
\endcode 
for a problem where two or more PDEs interact within the same physical
space, using a single domain for each PDE. The distribution process
for such problems typically requires further user intervention at the 
driver code level to ensure that such interactions continue to work in
parallel as they did in serial.

The individual meshes involved in the problem are distributed in exactly
the same way as a single-domain problem; therefore, each mesh will have
its own halo-haloed lookup scheme for elements and nodes.  This is
achieved by calling \c Mesh::distribute() for each mesh based upon the
relevant part of the Vector of unsigned integers returned by \c
Problem::partition_global_mesh().

There are two major types of interaction which can take place;
firstly, where two meshes occupy the same physical space but describe
different PDEs which interact with each other in some way (via source
terms in the equations, say), and secondly, where two domains interact
along a boundary (e.g. an FSI problem).  The next two sections
describe the processes required to ensure that such problems work when
run in parallel.

\subsubsection overlaying_meshes Overlaying meshes

An interaction that takes place between two meshes occupying the same
physical space is set up by calling the templated function
\code
template<class ELEMENT_0,class ELEMENT_1,unsigned EL_DIM_0,unsigned EL_DIM_1>
void Multi_domain_functions::set_sources
 (Problem* problem_pt, 
  Mesh* const &first_mesh_pt, Mesh* const &second_mesh_pt, 
  const unsigned& first_interaction=0, 
  const unsigned& second_interaction=0);
\endcode 
from within a problem constructor, after the global mesh has been
built but before the equation numbers are assigned.  Subsequently,
since the elements involved in the interaction (may) change once a 
problem has been distributed, we also ensure that this
function is called from the driver code in \c actions_after_distribute() 
(and \c actions_after_adapt() if any of the meshes are refineable).

The user must also ensure that each of \c ELEMENT_0 and \c ELEMENT_1 (the
element types within \c first_mesh_pt and \c second_mesh_pt
respectively, which are of dimension \c EL_DIM_0 and \c EL_DIM_1
respectively) both inherit from the class \c
ElementWithExternalElement (see \ref external_halo_elements).  The 
optional interaction parameters may be used if the interaction in
either direction is not the only interaction for either of the domains.

- See the \ref refineable_b_convection section for an example

\subsubsection meshes_with_boundary_interface Meshes which have a boundary interface

Problems where the domains do not overlap but interact along a
boundary already require the user to set the interaction up from
within the driver code, for example via the function
\code
FSI_functions::setup_fluid_load_for_solid_elements(...)
\endcode
The majority of the functionality which allows this to be called in
parallel is contained within this particular function and is
explained later in this document (see \ref multi_domain_details).  
The user is required to make a few modifications in order to allow a 
problem involving a call to this function to work in parallel, in 
addition to the modifications mentioned earlier for a single-domain 
problem (see \ref how_to_distribute).

- See the section on \ref fsi_examples for more details.

- If the solid mesh has the same dimension as the fluid mesh, then the
  processes involved are slightly more complex: see the \ref turek_flag
  for an example.

########################################################################
<HR>

\subsection distribute_details Further details of the distribute function

This section gives further details on the actual procedure used to
distribute a problem.  [Note: if \c distribute() is
called for a single-processor job, the library prints a warning
message and continues to the next stage without following any of the
subsequent steps; if \c distribute() is called and there are more
processors than elements in the global mesh then the library throws an
error.]

\subsubsection partitioning Partitioning the problem

The first method that takes place in \c Problem::distribute(...) is a
partition of all the elements in the global mesh (which is a combination
of all submeshes), via the virtual function
\code
Problem::partition_global_mesh(...)
\endcode
which, in its basic form, uses 
<a href="http://glaros.dtc.umn.edu/gkhome/views/metis">METIS</a> which
assigns each elements to a processor. This function 
returns a vector of unsigneds specifying which processor each 
element has been assigned to - the element partition.

If \c METIS returns a partition where no elements are assigned to a 
particular processor (this can take place, particularly when the 
number of processors is almost equal to the number of elements) then
we deal with this scenario after the partition has been returned from
\c METIS by ensuring that any processor that has no elements takes one 
of the elements from the processor that has the most elements, until
all processors have at least one element.

\subsubsection distributing_meshes Distributing the meshes

For each mesh involved, the vector of unsigneds returned from
\c partition_global_mesh() is then passed on to the function
\code
Mesh::distribute(...)
\endcode
For multiple meshes, the Vector is split into sections for each
submesh and then passed into the function.
In \c Mesh::distribute(), we set up a halo-haloed lookup scheme for
elements between processors that share element boundaries.  At the 
start of the routine all processors hold the whole mesh, and each 
processor retains from the elements within the mesh all those 
indicated by the Vector of unsigned integers detailing the partition, 
along with any elements which share nodes with these elements; such
elements are called halo elements.  All halo elements have an equivalent 
non-halo element on another processor, and these elements are called
haloed elements.  Thus, each mesh has the function
\code
Mesh::halo_element_pt(const unsigned& iproc, const unsigned& j)
\endcode
which returns a pointer to the \c jth halo element in the lookup
scheme between the current processor and process \c iproc, and also 
the function
\code
Mesh::haloed_element_pt(const unsigned& iproc, const unsigned& j)
\endcode
which returns a pointer to the \c jth element haloed by the current
process to the process \c iproc.

@I w 0.75\textwidth halo_haloed "Example of halo lookup scheme on two processors. "

Once the halo-to-haloed lookup scheme for elements is created, we mark
all nodes (on both non-halo elements and halo elements) that need to
be kept on each processor and remove any elements and nodes that
remain unmarked on any process. Following this, we create a
halo-haloed lookup scheme for nodes by calling the function
\code
Mesh::classify_halo_and_haloed_nodes(DocInfo& doc_info, 
                                     const bool report_stats);
\endcode
In this routine, we loop over all nodes on elements associated with
the current processor, and ensure that each nodes obtains a list of
processor numbers, populated with the current processor number, and, 
if it is also a halo element, the processor number relating to the
equivalent haloed element.  Once each node in the problem has been 
told the processors it is associated with, the highest-numbered of 
these processors is set to be the one that is "in charge" of the node.
Subsequently, any node on a processor that has a different
processor in charge of it must become a halo node (and its equivalent
haloed node is found in the node storage of the processor in charge).
We also ensure that any internal Data associated with halo elements is
itself halo.

After the distribution has taken place, we need to ensure that
after any mesh adaptation, the hanging status of equivalent halo-haloed
nodes is the same; in particular, any haloed nodes that change status
from hanging back to non-hanging may need to communicate this across
to their equivalent halo node.  This is achieved by calling the
routine
\code
RefineableMeshBase::synchronise_hanging_nodes(...)
\endcode
at the beginning of \c Problem::assign_eqn_numbers(...), which does
exactly what it says - ensuring that any discrepancy in the hanging
status between a halo-haloed node pair is removed.  In order to
achieve this, we need a further storage scheme which
detailed \b all the nodes shared between a pair of processors.  This was
necessary because a hanging node's master node may be on a different
(higher-numbered) processor to either of the processors involved in
the hanging node's halo-haloed lookup scheme, and therefore would not
be available via the halo-haloed lookup scheme.  However, such a
master node will be included in the shared node storage scheme, and 
this is where it is accessed from if it is required in order to alter 
the hanging status of any halo node where there is a discrepancy.

\subsubsection actions_routines Actions before and after distribute

In certain problems, it is more straightforward to only distribute
some of the meshes involved (particularly if some of the meshes are
of a lower dimension than the main "bulk" mesh) and deal with the
other meshes after the "main" distribution has taken place.  To
facilitate this, the virtual functions
\code
Problem::actions_before_distribute()
Problem::actions_after_distribute()
\endcode
are provided to allow the user to remove such meshes from the global
mesh before a distribution takes place.  Such meshes are usually built
from the bulk mesh within the specific driver code.

\subsubsection further_routines Further parallel modifications

Once the problem has been distributed, the equation numbers assigned to
and data held by haloed elements and nodes need to be copied over
to the equivalent halo elements and nodes on the other processor.
The equation numbers are synchronised across all processors by calling
\code
Problem::synchronise_eqn_numbers()
\endcode
once all equation numbers have been assigned on each processor.
This routine ensures that equation numbers assigned to non-halo
objects (Internal Data, Nodes, SolidNodes) on each processor are
distinct, and then copies the equation numbers of haloed objects to
the equivalent halo objects using the helper function
\code
Problem::copy_haloed_eqn_numbers_helper(Mesh* &mesh_pt);
\endcode
for each (sub)mesh involved in the problem.

All values held (including history and position values) for each of 
these haloed objects are synchronised with the equivalent halo objects 
by calling
\code
Problem::synchronise_dofs(Mesh* &mesh_pt)
\endcode
for each (sub)mesh after any point where a solve takes place 
within an iterative Newton solver loop.

\subsubsection face_elements Problems involving FaceElements

The minor modifications to allow a driver code where
\c FaceElements are used to function in parallel are made within the
build procedure for such meshes.  If, after a distribution has taken 
place, a bulk element becomes a halo element, then
any \c FaceElement which is "attached" to it in a consequent build is
also turned into a halo element.  The user has to
ensure that such "FaceMeshes" are re-built by re-calling the process
from the \c actions_after_distribute() function; see \ref
two_d_poisson_flux_bc_adapt for more details.

########################################################################
<HR>

\subsection multi_domain_details Further details on multi-domain method

\subsubsection external_halo_elements External elements

Any element which requires information from an element in another
"external" mesh must inherit from the base class \c
ElementWithExternalElement.  Within this base class we store 
a pointer to the "external" element at each integration point
of the current element and a Vector of the external element's local
coordinates at that integration point, ie:
\code
FiniteElement*& external_element_pt(const unsigned& interaction, const
unsigned& ipt);
\endcode
\code
Vector<double>& external_element_local_coord(const unsigned&
interaction, const unsigned& ipt);
\endcode
where the unsigned parameter \c interaction denotes the interaction
index (required in cases where an element has more than one element
with which it interacts, e.g. an FSI problem where a solid element is
affected by a fluid load on both its "front" and "back", see the 
<a href="../../../interaction/fsi_channel_with_leaflet/html/index.html">FSI 
channel with leaflet problem</a> for further details).

Any interaction between meshes involves information from one element
being passed into another element (for example, any solid element in an
FSI problem needs to know its adjacent fluid element(s) in order to
calculate the fluid load).  For a problem run on a single processor,
this "external" element (from the "external" (sub)mesh) will (clearly!)
always be available to the current element (on the current (sub)mesh).
However, this may not always be the case for a problem using multiple
processors - an "external" element may be on a different processor than
the current element.  If this is the case, then another lookup scheme
is introduced - the processor for the current element creates a copy
("external halo") of the element based upon information communicated to
it from the processor which contains the original ("external haloed")
element.  All nodes on an external halo element become external halo
nodes and have equivalent external haloed counterparts on the external
haloed elements.  

@I w 0.75\textwidth external_halo_elements "Example of external halo elements for the collapsible channel domain. "

In this figure, we are considering the collapsible channel problem,
where the external elements are those adjacent to the solid wall.  The
\c distribute() function in this case placed the entire solid mesh
onto processor 1 (indicated by the thick black line on the right half
of the figure), along with the indicated fluid elements, with the
remainder of the fluid elements on processor 0.  Since some of the
required "adjacent" elements only exist on processor 0 (the elements
indicated by thicker outlines on the left half of the figure), these
must become external halo elements on processor 1 (externally haloed 
elements from processor 0).  The remained of the required "adjacent" 
elements already exist on processor 1 and therefore become external
elements.

This lookup scheme then functions in the same way as the halo-haloed
lookup scheme during the assignment of equation numbers and the Newton
solver.  We ensure that the equation numbers in each case are
synchronised by calling the helper function
\code
Problem::copy_external_haloed_eqn_numbers_helper(Mesh* &mesh_pt);
\endcode
from within \c synchronise_eqn_numbers() for each (sub)mesh;
subsequently, during any solve, we call
\code
Problem::synchronise_external_dofs(Mesh* &mesh_pt);
\endcode
immediately after any call to \c synchronise_dofs().

\subsubsection locating_and_creating_external_elements Locating and creating external elements

External (halo) elements are set by calling the function
\code
template<class ELEMENT,class EXT_ELEMENT,class GEOM_OBJECT,
 unsigned EL_DIM_LAG,unsigned EL_DIM_EUL>
 void set_external_storage(Problem* problem_pt,
                           Mesh* const &mesh_pt,
                           Mesh* const &external_mesh_pt,
                           const unsigned& interaction_index=0,
                           Mesh* const &external_face_mesh_pt=0);
\endcode
in the \c Multi_domain_functions namespace, either from within \c
Multi_domain_functions::set_sources() for the case where meshes
overlay, or from within \c
FSI_functions::set_fluid_load_for_solid_elements() for FSI
problems. The arguments of this function are a pointer to the Problem,
then pointers to the current submesh, and the submesh in which the
current submesh will locate its external elements.  The final two
parameters are optional; the first can be set if there is more than one
interaction between the specific meshes, and the second is set
if a mesh of \c FaceElements is used to specify the interaction
between two meshes along a boundary (as is the case in all FSI
problems).  The templated parameters related
directly to the submeshes involved: ELEMENTs must be in the current
submesh (and inherit from ElementWithExternalElement), EXT_ELEMENTs
must be in the external submesh, and the remaining templated
parameters are those required to create a \c MeshAsGeomObject from the
external submesh or the external face mesh (depending on the type of
interaction)- a GEOM_OBJECT, and the Eulerian and Lagrangian
dimensions of the external submesh used to create the
\c MeshAsGeomObject.

The \c set_external_storage() function is split up into several helper
functions which follow the procedure required to locate, communicate
and create external (halo) elements and nodes.  

Firstly, a \c MeshAsGeomObject is created based upon the external (face)
mesh.  Once this is set up, each processor in turn communicates the
coordinates of the integration points of each of its elements in the
current mesh to each of the other processors.  Once every processor
holds the same set of coordinates, each coordinate in turn is located
in the \c MeshAsGeomObject by calling the function
\code
MeshAsGeomObject::locate_zeta(const Vector<double>& zeta, 
                              GeomObject*& sub_geom_object_pt, 
                              Vector<double>& s)
\endcode
simultaneously on every processor.  This returns a \c GeomObject (which
is a sub-\c GeomObject of the \c MeshAsGeomObject, i.e. the elements of the
mesh) and a local coordinate.  If these objects are returned on the
current processor then we are done; we set \c
external_element_pt() to be the sub-geometric object, and \c 
external_element_local_coord() to be the local coordinate.  If an
object is returned on another processor, then we communicate the
information required to create an exact copy of the object on the
current processor; this will become an external halo element (and the
original object an external haloed element).  Since MPI cannot deal
with sending objects such as \c Elements or \c Nodes directly, this involves
distilling the information required into a series of unsigned integers
and doubles which are read back on the current processor and used to
create new elements and nodes (and anything else that may be
associated with these elements and nodes, e.g. if they are \c SolidNode,
\c AlgebraicNode, etc.).

The \c MeshAsGeomObject::locate_zeta() function uses a bin search
method to locate the required element for the specified coordinate.
The bin structure is set up when the \c MeshAsGeomObject is first
created, and is a regular structure which is set to cover the
whole of the \c MeshAsGeomObject based upon the underlying mesh's minimum
and maximum coordinates in each dimension.  Each sub-object of the
\c MeshAsGeomObject is then assigned to one or more of the bins based
upon coordinates within the sub-object.  The locate procedure then
takes the coordinate passed into it and works out which bin is the
most likely candidate to contain the required sub-object containing
this coordinate.  It visits all sub-objects within this bin: if it is
successful (on any processor) then it returns the sub-object and the
corresponding local coordinate; if not, then it follows the
neighbouring bins in a spiralling pattern and visits all the
sub-objects until it successfully finds the correct sub-object.  The
number of bins in each dimension and the size of the bin structure in
each dimension may be specified by the user at the driver code level
using
\code
Multi_domain_functions::Setup_bins_again=true;
\endcode
along with, for example:
\code
Multi_domain_functions::Nx_bin=50;
Multi_domain_functions::X_min=0.0;
Multi_domain_functions::X_max=3.0;
\endcode
(and similarly for any other values along any other dimension).

Once all external (halo) elements and nodes have been succesfully
created between all pairs of processors it is still possible that
there may be duplication between two sets of storage.  This is
bypassed at the element level by ensuring that only non-halo elements
may become external halo elements; however, the same is not possible
for nodes.  We therefore use the function
\code
Multi_domain_functions::remove_duplicate_data(Mesh* const &mesh_pt);
\endcode
which ensures that any external halo-haloed nodes between a
pair of processors do not have a duplicate in any other set of
external halo-haloed nodes on another pair of processors.  This is
achieved by assigning only the global equation numbers, synchronising,
and then finding Data which have the same equation number assigned to
them and ensuring that their pointers match.

\subsubsection node_update Algebraic and macro node update

In an FSI problem, the solid elements are affected by the fluid
elements that are adjacent to them, but a fluid element may be
affected by a solid element that is not adjacent (e.g. all elements
within the collapsible section of the collapsible channel problem).  
These fluid elements are affected in the sense that their 
nodal coordinates are altered depending upon the motion of the solid 
mesh and its elements.  Such nodal coordinates may be updated in two
ways: (1) an algebraic node update: see the <a href="../../../interaction/fsi_collapsible_channel_algebraic/html/index.html">FSI
collapsible channel example with algebraic node updates</a> or (2) a macro-element based node update: again
see the <a href="../../../interaction/fsi_collapsible_channel/html/index.html">FSI
collapsible channel example</a>.  These procedures
continue to work in parallel problems due to the use of either the \c
keep_all_elements_as_halos() flag on a solid mesh which is to become
the \c MeshAsGeomObject that controls the node update procedure, or the
\c must_be_kept_as_halo() flag which keeps all elements required to
create any \c MeshAsGeomObject that controls the node update procedure.  In
the second case, the fluid mesh (either an \c AlgebraicMesh or a
\c MacroElementNodeUpdateMesh) must have access functions which allow the
user to reset the MeshAsGeomObjects which control the node update
after a distribution or an adaptation has taken place.


########################################################################
<HR>

\subsection examples Examples of parallel driver codes

The following examples illustrate the steps required to modify a
serial driver code so that it works in parallel.  In general, all
parallel drivers need to:

- call \c MPI_Helpers::init(argc,argv) and \c MPI_Helpers::finalize()
  at the beginning and end of \c main()
- modify the output file from \c problem.doc_solution() to include the
  current processor number to distinguish output from distinct
  processors
- call \c problem.distribute() at some point after the problem has
  been constructed, but before any non-uniform refinement takes place

\subsubsection adaptive_driven_cavity Adaptive driven cavity problem

To show how this works in
practice, we demonstrate how to modify the serial driver code for the
<a href="../../../navier_stokes/adaptive_driven_cavity/html/index.html">
adaptive driven cavity problem</a> to work in parallel.

Firstly, we detail the changes required in the main driver code.  We
first of all need to ensure that MPI is initialised and finalised;
this is achieved by calling the functions
\code
MPI_Helpers::init(argc,argv)
MPI_Helpers::finalize()
\endcode
at the beginning and end of the main body.  We then ensure that \c
Problem::distribute() is called before any non-uniform adaptation 
or solving takes place; so, for example, this driver code would
change from

\dontinclude adaptive_driven_cavity.cc
\skipline Solve problem with Taylor Hood elements
\until end of Taylor Hood elements

to

\dontinclude adaptive_driven_cavity_for_doc.cc
\skipline Solve problem with Taylor Hood elements
\until end of Taylor Hood elements

with a similar change for the problem using Crouzeix-Raviart elements.

In this particular driver code, the pressure is fixed to zero in the
"first" element.  Since the "first" element on each
processor will be different once the problem is distributed, we also
modify \c Problem::actions_after_adapt() to only fix the pressure in
what was the zeroth element in the non-distributed problem as follows:

\dontinclude adaptive_driven_cavity_for_doc.cc
\skipline After adaptation:
\until end_of_actions_after_adapt

This modification does not need to happen in the Problem constructor
as the distribution occurs after the problem has been constructed.

The fully modified parallel driver code can be found at
<CENTER>
<A HREF="../../../../demo_drivers/mpi/distribution/adaptive_driven_cavity/adaptive_driven_cavity.cc">
demo_drivers/mpi/distribution/adaptive_driven_cavity/adaptive_driven_cavity.cc
</A>
</CENTER>

[Note: This driver code, as with all parallel driver codes, uses the
form of \c Problem::distribute() which reads in a file determining the
partition of the elements if used in a validation run.]

For further examples of how this works in practice for both 
two-dimensional and three-dimensional single-domain problems, look in
the directory
<CENTER>
<A HREF="../../../../demo_drivers/mpi/distribution/">
demo_drivers/mpi/distribution/
</A>
</CENTER>

########################################################################
<HR>

\subsubsection two_d_poisson_flux_bc_adapt 2D Poisson problem with flux boundary conditions

Taking the 
<a href="../../../poisson/two_d_poisson_flux_bc_adapt/html/index.html">2D 
Poisson equation with flux boundary conditions</a> as our example this
time, we note that in this case the procedures we need to follow are
identical to those already followed by \c
Problem::actions_before_adapt() and \c Problem::actions_after_adapt()
and consequently, all that is required in addition to distributing the
problem as detailed above is the addition of

\dontinclude two_d_poisson_flux_bc_adapt.cc
\skipline Actions before distribute:
\until }
\skipline Actions after distribute:
\until }

to the problem constructor.

For full sources of this driver code see
<CENTER>
<A HREF="../../../../demo_drivers/mpi/distribution/two_d_poisson_flux_bc_adapt/two_d_poisson_flux_bc_adapt.cc">
demo_drivers/mpi/distribution/two_d_poisson_flux_bc_adapt/two_d_poisson_flux_bc_adapt.cc
</A>
</CENTER>

########################################################################
<HR>

\subsubsection refineable_b_convection Refineable Boussinesq convection problem

An interaction described in the previous section can take place between
two meshes that occupy the same physical space; for example, a
Boussinesq convection problem where there is an "interaction" between
the advection-diffusion equation and the Navier--Stokes equations in
the sense that each equation has a source term which depends upon 
parameters that are obtained from the solution of the other equation.
(The benefit of doing this, compared to combining the elements and
using a single domain, is that a single error estimator can be used
for each individual mesh of elements rather than some combined error 
estimator which has to take into account all of the spatial variables
in a combined element.)

In the Boussinesq example, the "external" elements contain information required
by the source terms in each set of equations:
<center>
\f[ \rho_{0} \left (\frac{\partial u^{*}_i}{\partial t^{*}} +
u^{*}_j \frac{\partial u^{*}_i}{\partial x^{*}_j} \right) =
- \frac{\partial p^{*}}{\partial x^{*}_i} +  \left[\rho_{0} -
  \alpha\rho_{0}(\theta^{*}
 - \theta_{0})\right] G^{*}_i + \mu_{0}
\frac{\partial }{\partial x^{*}_j} \left[
\frac{\partial u^{*}_i}{\partial x^{*}_j} +
\frac{\partial u^{*}_j}{\partial x^{*}_i} \right],
\f]
</center>
is the momentum equation with an additional buoyancy term dependent
upon temperature \f$ \theta^{*} \f$, and
<center>
\f[ \frac{\partial \theta^{*}}{\partial t^{*}} +
u^{*}_{j}\frac{\partial \theta^{*}}{\partial x^{*}_{j}} =
\kappa\frac{\partial}{\partial x^{*}_{j}}\left(\frac{\partial
\theta^{*}}{\partial
x^{*}_{j}}\right),\f]
</center>
is the advection-diffusion equation where the "wind" is given by the
fluid velocity \f$ u^{*}_j \f$.  [See 
<a href="../../../multi_physics/b_convection/html/index.html">the
tutorial on Boussinesq convection</a> for
further detail.]

We therefore take the advection-diffusion and Navier--Stokes elements
required for these sets of equations, and create new elements which
also inherit from the class \c ElementWithExternalElement.  Within
these new elements we overload the functions which provide these
source terms so that they get the required information directly from 
the "external" element.  In this instance, therefore, we overload \c
get_body_force_nst(...) and \c get_wind_adv_diff(...) as follows:

\dontinclude my_boussinesq_elements.h
\skipline start_of_get_body_force_nst
\until end of get_body_force_nst

\skipline start_of_get_wind_adv_diff
\until end of get_wind_adv_diff

For example driver codes for the distributed Boussinesq problem using 
both single- and multi-domain methods, see
<CENTER>
<A HREF="../../../../demo_drivers/mpi/multi_domain/boussinesq_convection">
demo_drivers/mpi/multi_domain/boussinesq_convection
</A>
</CENTER>

The same principle applies to any problem where two sets of
equations "interact" within the same physical space: (1) call the \c
set_sources(...) function to set up the interaction, and
(2) ensure that the functions which provide the relevant source terms
within these equations are overloaded to use the external elements
which were set by the call to \c set_sources(...).

\subsubsection fsi_examples FSI examples

In most of the FSI examples in \c oomph-lib the solid mesh has a lower 
dimension than the fluid mesh.  In these cases the modification
required to make a serial driver code work in parallel is straightforward:
when the solid mesh is turned into a \c MeshAsGeomObject, the user
needs to ensure that all the elements within the solid mesh are kept
available on every processor after a distribution has taken place.
The partition will determine which processor these elements should
initially be kept on; if the \c keep_all_elements_as_halos() flag for
the solid mesh is set to true, then these elements become halo
elements on every other processor involved.  This flag is set by
passing in an optional boolean parameter to the \c MeshAsGeomObject
constructor, e.g.:
\code
MeshAsGeomObject<1,2,FSIHermiteBeamElement>* wall_geom_object_pt= 
 new MeshAsGeomObject<1,2,FSIHermiteBeamElement>(Wall_mesh_pt);
\endcode

[Note: this will only work if the solid mesh used to create the
\c MeshAsGeomObject is not refined in any way; see the \ref turek_flag
example for how to deal with this scenario.]

The \c actions_after_distribute() function, as with the \c
actions_after_adapt() function, must reset anything that could have
been changed by a modification of a mesh or its elements and nodes.
In most cases this is simply achieved by calling \c
actions_after_adapt() from \c actions_after_distribute() as in the
earlier example (see \c actions_routines).

A few further modifications to the order in which functions are called
are also necessary: any auxiliary node update functions must be set
\b after a call to \c setup_fluid_load_info_for_solid_elements() from 
within \c actions_after_adapt() or \c actions_after_distribute().

Examples of modified driver codes for FSI problems for a collapsible
channel, an oscillating ring and a leaflet loaded from both sides by a
fluid can be found in
<CENTER>
<A HREF="../../../../demo_drivers/mpi/multi_domain/">
demo_drivers/mpi/multi_domain
</A>
</CENTER>

########################################################################
<HR>

\subsubsection turek_flag Turek flag problem

There are a few further complications involved if the solid and fluid
mesh are of the same dimension, or if the solid mesh is itself
refineable; as an example, we revisit 
<A HREF="../../../interaction/turek_flag/html/index.html">Turek and Hron's FSI benchmark problem</a>.  
This particular parallel driver has been modified further from the original
serial version to allow the solid mesh to refine as well as the fluid
mesh; we explain here the changes required within the 
<a href="../../../interaction/turek_flag/html/index.html">original
driver code</a> to allow for both refinement of the solid mesh and the
distribution of the whole problem.

Firstly, as with all problems involving meshes built from traction
elements, \c actions_before_distribute() must ensure that such meshes
are removed from the global mesh storage before the distribution takes
place.  We must also ensure that the solid elements used to build
these traction elements are kept on every processor.  This is achieved
by looping over the traction meshes and setting the flag \c
GeneralisedElement::must_be_kept_as_halo() for all their elements to
true, giving the following:

\dontinclude turek_flag.cc
\skipline start_of_actions_before_distribute
\until end of actions before distribute

[Note: the traction meshes have not yet been deleted!  This is
explained in more detail later...]

In \c actions_after_distribute() we need to delete the traction
elements and re-attach them to the newly distributed solid mesh.  This
required an additional function to perform the deleting:

\skipline start_of_delete_traction_elements
\until end of delete traction elements

to complement the already-exisiting \c create_fsi_traction_elements()
function.  The \c actions_after_distribute therefore reads as follows:

\dontinclude turek_flag.cc
\skipline start_of_actions_after_distribute
\until build of FSISolidTractionElements is complete

Now that the traction meshes have been rebuilt, we create new
\c MeshAsGeomObjects from them and tell the (algebraic) fluid mesh:

\until set_tip_flag_pt

As the sub-\c GeomObjects of these \c MeshAsGeomObjects may have changed, we
must now call the \c update_node_update() function again for each
node in the fluid mesh:

\until }

Now we add the traction meshes back to the problem and rebuild the
global mesh:

\until rebuild_global_mesh

Finally, we can now re-set the fluid load on the solid elements for
each of the relevant boundaries within the fluid mesh, and then re-set
the auxiliary node update function:

\until end of actions after distribute

For the complete modified driver code, see
<CENTER>
<A HREF="../../../../demo_drivers/mpi/multi_domain/turek_flag/turek_flag.cc">
demo_drivers/mpi/multi_domain/turek_flag/turek_flag.cc
</A>
</CENTER>


