//LIC// ====================================================================
//LIC// This file forms part of oomph-lib, the object-oriented, 
//LIC// multi-physics finite-element library, available 
//LIC// at http://www.oomph-lib.org.
//LIC// 
//LIC//           Version 0.85. June 9, 2008.
//LIC// 
//LIC// Copyright (C) 2006-2008 Matthias Heil and Andrew Hazel
//LIC// 
//LIC// This library is free software; you can redistribute it and/or
//LIC// modify it under the terms of the GNU Lesser General Public
//LIC// License as published by the Free Software Foundation; either
//LIC// version 2.1 of the License, or (at your option) any later version.
//LIC// 
//LIC// This library is distributed in the hope that it will be useful,
//LIC// but WITHOUT ANY WARRANTY; without even the implied warranty of
//LIC// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
//LIC// Lesser General Public License for more details.
//LIC// 
//LIC// You should have received a copy of the GNU Lesser General Public
//LIC// License along with this library; if not, write to the Free Software
//LIC// Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
//LIC// 02110-1301  USA.
//LIC// 
//LIC// The authors may be contacted at oomph-lib@maths.man.ac.uk.
//LIC// 
//LIC//====================================================================
//Include guards
#ifndef OOMPH_BLOCK_PRECONDITION_HEADER
#define OOMPH_BLOCK_PRECONDITION_HEADER


// Config header generated by autoconfig
#ifdef HAVE_CONFIG_H
#include <oomph-lib-config.h>
#endif

// c++ include
#include <list>
#include <math.h>

// oomph-lib includes
#include "matrices.h"
#include "mesh.h"
#include "problem.h"
#include "preconditioner.h"
#include "SuperLU_preconditioner.h"

namespace oomph
{


 //============================================================================
 /// Block Preconditioner base class. The block structure of the
 /// overall problem is determined from the \c Mesh's constituent
 /// elements. Each constituent element must be block-preconditionable - i.e 
 /// must implement the \c GeneralisedElemens functions \c nblock_types() and 
 /// get_block_numbers_for_unknowns(...). A \c Problem can have several
 /// \c Meshes, but each \c Mesh can only contain a single type of element.
 /// The association between global degrees of freedom and their unique global 
 /// dof numbers is therefore based on information provided by the elements. 
 /// \n\n
 /// By default each type of DOF is assumed to be unqiue type of block,
 /// but DOF types can be grouped together in a single block when 
 /// block_setup(...) is called.
 /// \n\n
 /// This class can function in one of two ways. Either it acts as a
 /// stand-alone block preconditioner which computes and stores 
 /// the association between global degrees of freedom and their unique global 
 /// block numbers itself. Alternatively, the block preconditioner can act as 
 /// a subsidiary block preconditioner within a (larger) master block 
 /// preconditioner (pointed to by Master_block_preconditioner_pt). 
 /// The master block preconditioner 
 /// must have an equal or greater number of block types. Examples
 /// are the FSI precondititioner which is the 3x3 "master block preconditioner"
 /// for the Navier-Stokes preconditioners which deals with the
 /// 2x2 fluid-blocks within the overall structure. In this case, \b only
 /// the master block preconditioner computes and stores the master
 /// lookup schemes. All block preconditioners compute and store their own
 /// optimised lookup schemes.
 /// \n\n
 /// In cases where a \c Problem contains elements of different element types 
 /// (e.g. fluid and solid elements in a fluid-structure interaction problem), 
 /// access to the elements of the same type must be provided via pointers to 
 /// (possibly auxiliary) \c Meshes that only contain elements of a single 
 /// type. The block preconditioner will then create global block
 /// numbers by concatenating the block types. Consider, e.g. a fluid-structure
 /// interaction problem in which the first \c Mesh contains (fluid) 
 /// elements whose degrees of freedom have been subdivided into 
 /// types "0" (the velocity, say) and "1" (the pressure say), while 
 /// the second \c Mesh contains (solid) elements whose degrees of freedom
 /// are the nodal displacements, classified as its type "0". 
 /// The combined block preconditioner then has three "block types": 
 /// "0": Fluid velocity, "1": Fluid pressure, "2": Solid nodal positions.
 /// NOTE: currently this preconditioner uses the same communicator as the 
 /// underlying problem. We may need to change this in the future.
 //============================================================================
 template<typename MATRIX>
  class BlockPreconditioner : public virtual Preconditioner
  {

   public :

    /// \short Constructor
    BlockPreconditioner()
    {
     // Initially set the master block preconditioner pointer to zero
     // indicating that this is stand-alone preconditioner that will
     // set up its own block lookup schemes etc.
     Master_block_preconditioner_pt = 0;

     // Initialise number of rows in this block preconditioner.
     // This information is maintained if used as subsidiary or
     // stand-alone block preconditoner (in the latter case it
     // obviously stores the number of rows within the subsidiary
     // preconditioner
     Nrow=0;

     // Initialise number of different block types in this preconditioner. 
     // This information is maintained if used as subsidiary or
     // stand-alone block preconditoner (in the latter case it
     // obviously stores the number of rows within the subsidiary
     // preconditioner 
     Nblock_types=0;
     Ndof_types=0;

#ifdef OOMPH_HAS_MPI
     // initially set the number of rows of the lookup scheme stored
     // by this preconditioner to zero.
     // Note these variables are only used by the master preconditioner
     Min_global_index = 0;
     Nglobal_indices = 0;
#endif
    }

   /// Destructor (empty)
   virtual ~BlockPreconditioner()
    {   
     this->clean_memory();
    }

   /// Broken copy constructor
   BlockPreconditioner(const BlockPreconditioner&)
    {
     BrokenCopy::broken_copy("BlockPreconditioner");
    }

   /// Broken assignment operator
   void operator=(const BlockPreconditioner&)
    {
     BrokenCopy::broken_assign("BlockPreconditioner");
    }

   /// \short Function to turn this preconditioner into a 
   /// subsidiary preconditioner that operates within a bigger
   /// "master block preconditioner (e.g. a Navier-Stokes 2x2 block
   /// preconditioner dealing with the fluid sub-blocks within a
   /// 3x3 FSI preconditioner. Once this is done the master block
   /// preconditioner deals with the block setup etc. \n
   /// The vector block_map must specify the block number in the 
   /// master preconditioner that corresponds to a block number in this 
   /// preconditioner.\n 
   /// \b 1. The length of the vector is used to determine the number of 
   /// blocks in this preconditioner therefore it must be correctly sized. \n
   /// \b 2. block_setup(...) should be called in the master preconditioner
   /// before this method is called. \n
   /// \b 3. block_setup(...) should be called in the corresponding subsidiary 
   /// preconditioner after this method is called. 
   void turn_into_subsidiary_block_preconditioner
    (BlockPreconditioner<MATRIX>* master_block_prec_pt,
     Vector<unsigned>& block_map);

    protected:

   /// \short Determine the size of the matrix blocks and setup the
   /// lookup schemes relating the global degrees of freedom with
   /// their "blocks" and their indices (row/column numbers) in those
   /// blocks.\n
   /// The distributions of the preconditioner and the blocks are
   /// automatically specified (and assumed to be uniform) at this
   /// stage.\n
   /// This method should be used if each DOF type corresponds to a 
   /// unique block type.
   void block_setup(Problem* problem_pt, DoubleMatrixBase* matrix_pt);

   /// \short Determine the size of the matrix blocks and setup the
   /// lookup schemes relating the global degrees of freedom with
   /// their "blocks" and their indices (row/column numbers) in those
   /// blocks.\n
   /// The distributions of the preconditioner and the blocks are
   /// automatically specified (and assumed to be uniform) at this
   /// stage.\n
   /// This method should be used if each DOF type corresponds to a 
   /// unique block type. 
   /// This method should be used if each any block contains more than one
   /// type of DOF. The argument vector dof_to_block_map should be of length
   /// ndof. Each element should contain an integer indicating the block number
   /// corresponding to that type of DOF.
   void block_setup(Problem* problem_pt, DoubleMatrixBase* matrix_pt,
                    Vector<unsigned> dof_to_block_map);

   /// \short Gets block (i,j) from the original matrix, pointed to by
   /// matrix_pt and returns it in block_matrix_pt
   void get_block(const unsigned& i, const unsigned& j, 
                  MATRIX* matrix_pt,
                  MATRIX*& block_matrix_pt);

   /// \short Get all the block matrices required by the block preconditioner.
   /// Takes the pointer to the original matrix and a matrix of bools
   /// that indicate if a specified sub-block is required
   /// for the preconditioning operation. Computes the
   /// required block matrices, and stores pointers to them
   /// in the matrix block_matrix_pt. If an entry in block_matrix_pt
   /// is equal to NULL on return, that sub-block has not been requested and is
   /// therefore not available.
   void get_blocks(MATRIX* matrix_pt,
                   DenseMatrix<bool>& required_blocks,
                   DenseMatrix<MATRIX*>& block_matrix_pt);

   /// \short Assemble the block preconditioner as a single matrix. This is
   /// useful because in some cases the block preconditioner cannot be applied
   /// by linear-algebra-style block elimination, operating on
   /// individual sub-blocks; this function takes the matrix
   /// of block pointers and returns a single matrix containing all the
   /// blocks of the matrix of blocks in a single matrix that can
   /// be solved directly.
   void build_preconditioner_matrix(DenseMatrix<MATRIX*>& block_matrix_pt,
                                    MATRIX*& preconditioner_matrix);

   /// \short Takes the naturally ordered vector and rearranges it into a 
   /// vector of sub vectors corresponding to the blocks, so s[b][i] contains
   /// the i-th entry in the vector associated with block b.
   /// Note: If the preconditioner is a subsidiary
   /// preconditioner then only the sub-vectors associated with 
   /// the blocks of the subsidiary preconditioner will be included. Hence
   /// the length of v is master_nrow() whereas the total length of the s
   /// s vectors is Nrow.
   void get_block_vectors(const DoubleVector& v,
                          Vector<DoubleVector >& s);

   /// \short Takes the vector of block vectors, s, and copies its entries
   /// into the  naturally ordered vector, v. If this is a subsidiary
   /// block preconditioner only those entries in v that are 
   /// associated with its blocks are affected.
   void return_block_vectors(const Vector<DoubleVector >& s, 
                             DoubleVector& v);

   /// \short Takes the naturally ordered vector, v, and extracts
   /// the n-th block vector, b. Here n is the block number in the 
   /// current preconditioner.
   void get_block_vector(const unsigned& n, const DoubleVector& v, 
                         DoubleVector& b);

   /// \short Takes the n-th block ordered vector, b,  and copies its entries
   /// to the appropriate entries in the naturally ordered vector, v.
   /// Here n is the block number in the current block preconditioner.
   /// If the preconditioner is a subsidiary block preconditioner
   /// the other entries in v  that are not associated with it
   /// are left alone
   void return_block_vector(const unsigned& n, 
                            const DoubleVector& b,
                            DoubleVector& v);

   /// \short Given the naturally ordered vector, v, return
   /// the vector rearranged in block order in w.
   void get_block_ordered_preconditioner_vector(const DoubleVector& v,
                                                DoubleVector& w);

   /// \short Takes the naturally ordered vector, w, and reorders it in 
   /// block order. Reordered vector is returned in v. Note: If the 
   /// preconditioner is a subsidiary
   /// preconditioner then only the components of the vector associated with 
   /// the blocks of the subsidiary preconditioner will be included. Hence
   /// the length of w is master_nrow() whereas that of the v is
   void return_block_ordered_preconditioner_vector(const DoubleVector& w, 
                                                   DoubleVector& v);

   /// \short return the number of block types
   unsigned nblock_types() const
    {
     return Nblock_types;
    }

   /// \short return the number of DOF types
   unsigned ndof_types() const
    {
     if (this->Master_block_preconditioner_pt != 0)
      {
       return Ndof_types;
      }
     else
      {
       unsigned nmesh = Mesh_pt.size();
       unsigned ndof = 0;
       for (unsigned i = 0; i < nmesh; i++)
        {
         ndof += Mesh_pt[i]->element_pt(0)->ndof_types();
        }
       return ndof;
      }
    }

   /// \short Access to i-th mesh (of the various meshes that
   /// contain block preconditionable elements of the same type)
   Mesh* mesh_pt(const unsigned& i) const
    {
     return Mesh_pt[i];
    }

   /// \short return the block number corresponding to a global index i_dof
   int block_number(const unsigned& i_dof) const
    {
     int dn = dof_number(i_dof);
     if (dn == -1)
      {
       return dn;
      }
     else
      {
       return Dof_number_to_block_number_lookup[dn];
      }
    }

   /// \short return the index in the block corresponding to a global block 
   /// number i_dof
   int index_in_block(const unsigned i_dof) const
    {

     // the index in the dof block
     unsigned index = index_in_dof(i_dof);

     // the dof block number
     int dof_block_number = dof_number(i_dof);
     if (dof_block_number >= 0)
      {

       // the 'actual' block number
       unsigned blk_number = block_number(i_dof);

       // compute the index in the block
       unsigned j = 0;
       while (int(Block_number_to_dof_number_lookup[blk_number][j]) != 
              dof_block_number)
        {
         index += 
          dof_block_dimension
          (Block_number_to_dof_number_lookup[blk_number][j]);
         j++;
        }

       // and return
       return index;
      }
     return -1;
    }

   /// \short Returns the first global index for which there is 
   /// block mapping data. \n
   /// In parallel the each processor only holds a subset of the 
   /// global index to block index lookup scheme data.
   /// The methods block_number(...) and index_in_block(...) will only
   /// work for global indices between first_lookup_row() and 
   /// first_lookup_row()+nlookup_rows()
   const unsigned first_lookup_row() { return Min_global_index; }

   /// \short Returns the number of global indices for which there is 
   /// block mapping data. \n
   /// In parallel the each processor only holds a subset of the 
   /// global index to block index lookup scheme data.
   /// The methods block_number(...) and index_in_block(...) will only
   /// work for global indices between first_lookup_row() and 
   /// first_lookup_row()+nlookup_rows()
   const unsigned nlookup_rows() { return Nglobal_indices; }

   /// \short What is the size of  the "block" i, i.e. 
   // how many degrees of freedom
   /// are associated with it? Note that if this preconditioner
   /// acts as a subsidiary preconditioner, then i refers to the 
   /// block number in the subsidiary preconditioner not the 
   /// master block preconditioner
   unsigned block_dimension(const unsigned& b) const
    {
     return Block_distribution_pt[b]->nrow();
    }

   /// \short access function to the block distributions
   const LinearAlgebraDistribution*
    block_distribution_pt(const unsigned b) const
    {
     return Block_distribution_pt[b];
    }

   /// \short access to the problem pt
   const Problem* problem_pt() const { return Problem_pt; }

   /// \short access to thte distribution of the master preconditioner. If this
   /// preconditioner does not have a master preconditioner then the
   /// distribution of this preconditioner is returned
   const LinearAlgebraDistribution* master_distribution_pt() const
    {
     if (Master_block_preconditioner_pt == 0)
      {
       return this->Distribution_pt;
      }
     else
      {
       return Master_block_preconditioner_pt->master_distribution_pt();
      }
    }

   /// \short A helper method to reduce the memory requirements of block 
   /// preconditioners. Once the methods get_block(...), get_blocks(...)
   /// and build_preconditioner_matrix(...) have been called in this and 
   /// all subsidiary block preconditioners this method can be called to 
   /// reduce the memory requirement.
   void post_block_matrix_assembly_partial_clean_memory()
    {
     if (Master_block_preconditioner_pt == 0)
      {
       Index_in_dof_block.clear();
       Dof_number.clear();
       Dof_dimension.clear();
      }
     Ndof_in_block.clear();
     Dof_number_to_block_number_lookup.clear();
     Block_number_to_dof_number_lookup.clear();
    }


   /// \short Vector of pointers to the meshes containing the elements used 
   /// in the block preconditioner. 
   Vector<Mesh*> Mesh_pt;

   /// \short Clears all BlockPreconditioner data. Called by the destructor
   /// and the block_setup(...) methods
   void clean_memory()
    {

     // clear the Distributions
     Distribution_pt->clear();
     unsigned nblock = Block_distribution_pt.size();
     for (unsigned b = 0; b < nblock; b++)
      {
       delete Block_distribution_pt[b];
      }
     Block_distribution_pt.resize(0);

     // clear the global index
     Global_index.clear();

     // call the post block matrix assembly clear
     this->post_block_matrix_assembly_partial_clean_memory();

#ifdef OOMPH_HAS_MPI
     // storage if the matrix is distributed
     unsigned nr = Rows_to_send_for_get_block.nrow();
     unsigned nc = Rows_to_send_for_get_block.ncol();
     for (unsigned p = 0; p < nc; p++)
      {
       delete[] Rows_to_send_for_get_ordered[p];
       delete[] Rows_to_recv_for_get_ordered[p];
       for (unsigned b = 0; b < nr; b++)
        {
         delete[] Rows_to_recv_for_get_block(b,p);
         delete[] Rows_to_send_for_get_block(b,p);
        }
      }
     Rows_to_recv_for_get_block.resize(0,0);
     Nrows_to_recv_for_get_block.resize(0,0);
     Rows_to_send_for_get_block.resize(0,0);
     Nrows_to_send_for_get_block.resize(0,0);
     Rows_to_recv_for_get_ordered.clear();
     Nrows_to_recv_for_get_ordered.clear();
     Rows_to_send_for_get_ordered.clear();
     Nrows_to_send_for_get_ordered.clear();

     // zero
     Min_global_index = 0;
     Nglobal_indices = 0;
#endif

     // zero
     if (Master_block_preconditioner_pt == 0)
      {
       Nrow = 0;
       Ndof_types = 0;
       Nblock_types = 0;
      }
    }

    private:

   /// \short Private helper function to check that every element in the block 
   /// matrix (i,j) matches the corresponding element in the original matrix
   void block_matrix_test(const MATRIX* matrix_pt,
                          const unsigned& i,
                          const unsigned& j,
                          const MATRIX* block_matrix_pt);

    public:

   /// \short Which block is the global unknown i_dof associated with? If this
   /// preconditioner is a subsidiary block preconditioner then the 
   /// block number in the subsidiary block preconditioner is returned. 
   /// If a particular  global DOF is not associated with this preconditioner 
   /// then -1 is returned
   int dof_number(const unsigned& i_dof) const
    {
     // I'm a stand-alone block preconditioner
     if (Master_block_preconditioner_pt == 0)
      {
#ifdef OOMPH_HAS_MPI
#ifdef PARANOID
       if (!(i_dof >= Min_global_index &&
             (i_dof < Min_global_index + Nglobal_indices)))
        {
         std::ostringstream error_message;
         error_message 
          << "Requested dof_number(...) for global DOF " << i_dof << ".\n"
          << "This processor only contains lookup schemes for global DOFS "
          <<  "between " << Min_global_index << " and " 
          << Min_global_index + Nglobal_indices -1 << "." << std::endl;
         throw OomphLibError(
          error_message.str(),
          "BlockPreconditioner::dof_number(...)",
          OOMPH_EXCEPTION_LOCATION);
        }
#endif
       return static_cast<int>(Dof_number[i_dof-Min_global_index]);
#else
       return static_cast<int>(Dof_number[i_dof]);
#endif
      }
     // else this preconditioner  is a subsidiary one, and its Block_number
     // lookup schemes etc haven't been set up
     else
      {
       // Block number in master prec
       unsigned blk_num = Master_block_preconditioner_pt->dof_number(i_dof);

       // Search through the Block_number_in_master_preconditioner for master
       // block blk_num and return the block number in this preconditioner 
       for (unsigned i = 0; i < this->ndof_types(); i++)
        {
         if (Dof_number_in_master_preconditioner[i] == blk_num)
          {return static_cast<int>(i);}
        }
       // if the master block preconditioner number is not found return -1
       return -1;
      }
    }

   /// \short What's the index (i.e. the row/column number) of global
   /// unknown i_dof within its block?
   unsigned index_in_dof(const unsigned& i_dof) const
    {
     if (Master_block_preconditioner_pt == 0)
      {
#ifdef OOMPH_HAS_MPI
#ifdef PARANOID
       if (!(i_dof >= Min_global_index &&
             (i_dof < Min_global_index + Nglobal_indices)))
        {
         std::ostringstream error_message;
         error_message 
          << "Requested index_in_dof(...) for global DOF " << i_dof << ".\n"
          << "This processor only contains lookup schemes for global DOFS "
          <<  "between " << Min_global_index << " and " 
          << Min_global_index + Nglobal_indices -1 << "." << std::endl;
         throw OomphLibError(
          error_message.str(),
          "BlockPreconditioner::index_in_dof(...)",
          OOMPH_EXCEPTION_LOCATION);
        }
#endif
       return Index_in_dof_block[i_dof-Min_global_index];
#else
       return Index_in_dof_block[i_dof];
#endif
      }
     else
      {
       return Master_block_preconditioner_pt->index_in_dof(i_dof);
      }
    }

   /// \short What is the size of  the dof "block" i, i.e. 
   /// how many degrees of freedom  are associated with it? 
   /// Note that if this preconditioner
   /// acts as a subsidiary preconditioner, then i refers to the 
   /// block number in the subsidiary preconditioner not the 
   /// master block preconditioner
   unsigned dof_block_dimension(const unsigned& i) const
    {
     if (Master_block_preconditioner_pt == 0)
      {
       return Dof_dimension[i];
      }
     else
      {
       unsigned master_i = master_dof_number(i);
       return Master_block_preconditioner_pt->dof_block_dimension(master_i);
      }
    }

   /// \short Returns the number of DOFs (number of rows or columns) in the 
   /// overall problem. [The prefix "master_" is sort of redundant
   /// when used as a stand-alone block preconditioner but is required to avoid
   /// ambiguities when there are master and subsidiary block preconditioners.
   /// In that case it's important to distinguish between the number of
   /// unknowns (rows) in the global problem and the number of rows
   /// involved in the subsidiary block preconditioner. The latter is stored
   /// (and maintained) separately for each specific block preconditioner 
   /// regardless of its role.
   unsigned master_nrow()
    {
     if (Master_block_preconditioner_pt == 0)
      {
       return Nrow;
      }
     else
      {
       return (this->Master_block_preconditioner_pt->master_nrow());
      }
    }

   /// \short Takes the block number within this preconditioner and returns the
   /// corresponding block number in the master preconditioner. If this 
   /// preconditioner does not have a master block preconditioner then the 
   /// block number passed is returned
   unsigned master_dof_number(const unsigned& b) const
    {
     if (Master_block_preconditioner_pt == 0)
      {
       return b;
      }
     else
      {
       return Master_block_preconditioner_pt->master_dof_number(
        Dof_number_in_master_preconditioner[b]);
      }
    }

   /// \short Number of DOFs (# of rows or columns in the full matrix) in this 
   /// preconditioner. NOTE:
   /// This information is maintained if used as subsidiary or
   /// stand-alone block preconditoner (in the latter case it
   /// obviously stores the number of rows within the subsidiary
   /// preconditioner)
   unsigned Nrow; 

   /// \short Number of different block types in this preconditioner. NOTE:
   /// This information is maintained if used as subsidiary or
   /// stand-alone block preconditoner (in the latter case it
   /// obviously stores the number of blocks within the subsidiary
   /// preconditioner) 
   unsigned Nblock_types;

   ///\short Number of different degree of freedom types in this preconditioner
   /// This information is maintained if used as subsidiary or
   /// stand-alone block preconditoner (in the latter case it
   /// obviously stores the number of dofs within the subsidiary
   /// preconditioner) 
   unsigned Ndof_types;

   /// \short Pointer to a master block preconditioner if the block 
   /// preconditioner is acting a sub block preconditioner. 
   /// NOTE : if the preconditioner does not have a master block preconditioner
   /// then this  pointer remains null.
   BlockPreconditioner<MATRIX>* Master_block_preconditioner_pt;

   /// \short For block number i in this preconditioner this vector contains
   /// the block number in the master preconditioner. Otherwise empty
   Vector<unsigned> Dof_number_in_master_preconditioner;

   /// \short Vector to store the mapping from the global dof number
   ///  to the index (row/colum number) within its block (Empty if this 
   /// preconditioner has a master preconditioner as this information is 
   /// obtained from the master preconditioner)
   Vector<unsigned> Index_in_dof_block;

   /// \short Vector to store the mapping from the global dof number to
   /// its block (Empty if this preconditioner has a master preconditioner as 
   /// this information is obtained from the master preconditioner)
   Vector<unsigned> Dof_number;

   /// \short Vector containing the size of each block, i.e. the number of
   /// global dofs associated with it. (Empty if this preconditioner has a 
   /// master preconditioner as this information is obtain from the master 
   /// preconditioner)
   Vector<unsigned> Dof_dimension;

   /// \short vectors of vectors for the mapping from block number / block DOF
   /// to global row number. (Empty if this preconditioner has a 
   /// master preconditioner as this information is obtain from the master 
   /// preconditioner) 
   Vector<Vector<unsigned> > Global_index;

   /// \short Vector to store the mapping from block number of DOF number
   Vector<Vector<unsigned> > Block_number_to_dof_number_lookup;

   /// \short Vector to the mapping from DOF number to block Number
   Vector<unsigned> Dof_number_to_block_number_lookup;

   /// \short number of types of degree of freedom associated with a particular
   /// block
   Vector<unsigned> Ndof_in_block;

   /// \short Storage for a pointer to the underlying problem. Set by 
   /// block-setup(...) and predominantly used to access the unlerlying
   /// communicator.
   Problem* Problem_pt;

   /// \short storage for the default distribution for each block
   Vector<LinearAlgebraDistribution*> Block_distribution_pt;

#ifdef OOMPH_HAS_MPI
   /// \short The global rows to be sent of block b to 
   /// processor p (matrix indexed [b][p])
   DenseMatrix<int*> Rows_to_send_for_get_block;

   /// \short The number of global rows to be sent of block b to 
   /// processor p (matrix indexed [b][p])
   DenseMatrix<unsigned> Nrows_to_send_for_get_block;

   /// \short the block rows to be recieved from processor p for block b
   /// (matrix indexed [b][p])
   DenseMatrix<int*> Rows_to_recv_for_get_block;

   /// \short the number of block rows to be recieved from processor p for 
   /// block b (matrix indexed [b][p])
   DenseMatrix<unsigned> Nrows_to_recv_for_get_block;

   /// \short the global rows to be sent to processor p for 
   /// get_block_ordered_... type methods
   Vector<int*> Rows_to_send_for_get_ordered;

   /// \short the number global rows to be sent to processor p for 
   /// get_block_ordered_... type methods
   Vector<unsigned> Nrows_to_send_for_get_ordered;

   /// \short the preconditioner rows to be received from processor p for 
   /// get_block_ordered_... type methods
   Vector<int*> Rows_to_recv_for_get_ordered;

   /// \short the number of preconditioner rows to be received from processor 
   /// p for get_block_ordered_... type methods
   Vector<unsigned> Nrows_to_recv_for_get_ordered;
#endif

   /// \short the minimum global index for which index_in_dof(...) and
   /// dof_number(...) is accessible on this processor.\n
   /// For distributed matrices the only lookup schemes for the row / column
   /// indices on a particular processor are stored. \n
   /// The variable is only used by the master preconditioner
   unsigned Min_global_index;

   /// \short the number of global indices for which index_in_dof(...) and
   /// dof_number(...) is accessible on this processor.\n
   /// For distributed matrices the only lookup schemes for the row / column
   /// indices on a particular processor are stored. \n
   /// The variable is only used by the master preconditioner
   unsigned Nglobal_indices;
  };



 ////////////////////////////////////////////////////////////////////////////
 ////////////////////////////////////////////////////////////////////////////
 ////////////////////////////////////////////////////////////////////////////



 //============================================================================
 /// \short Function to turn this preconditioner into a 
 /// subsidiary preconditioner that operates within a bigger
 /// "master block preconditioner (e.g. a Navier-Stokes 2x2 block
 /// preconditioner dealing with the fluid sub-blocks within a
 /// 3x3 FSI preconditioner. Once this is done the master block
 /// preconditioner deals with the block setup etc. \n
 /// The vector block_map must specify the block number in the 
 /// master preconditioner that corresponds to a block number in this 
 /// preconditioner.\n 
 /// \b 1. The length of the vector is used to determine the number of 
 /// blocks in this preconditioner therefore it must be correctly sized. \n
 /// \b 2. block_setup(...) should be called in the master preconditioner
 /// before this method is called. \n
 /// \b 3. block_setup(...) should be called in the corresponding subsidiary 
 /// preconditioner after this method is called. 
 //============================================================================
 template<typename MATRIX>
  void BlockPreconditioner<MATRIX>::turn_into_subsidiary_block_preconditioner
  (BlockPreconditioner<MATRIX>* master_block_prec_pt,
   Vector<unsigned>& block_map)
  {
   // Set the master block preconditioner pointer
   Master_block_preconditioner_pt = master_block_prec_pt;

   // get the number of block types in this preconditioner from the length
   // of the block_map vector
   Ndof_types =  block_map.size();
   Nblock_types = Ndof_types;

   // copy the block_map vector to the Block_number_in_master_preconditioner
   // vector used to store this information
   Dof_number_in_master_preconditioner.resize(Nblock_types);
   for (unsigned i = 0; i < Nblock_types; i++)
    {
     Dof_number_in_master_preconditioner[i] = block_map[i];
    }

   // compute number of rows in this (sub) preconditioner
   Nrow = 0;
   for (unsigned b = 0; b < Ndof_types; b++)
    {
     Nrow += this->dof_block_dimension(b);
    }

#ifdef PARANOID
   if (Nrow==0)
    {
     std::ostringstream error_message;
     error_message 
      << "Nrow=0 in subsidiary preconditioner. This seems fishy and\n"
      << "suggests that block_setup() was not called for the \n"
      << "master block preconditioner before turning this one into \n"
      << "a subsidiary one\n";
     throw OomphLibWarning(
      error_message.str(),
      "BlockPreconditioner::turn_into_subsidiary_block_preconditioner(...)",
      OOMPH_EXCEPTION_LOCATION);
    }
#endif
  }


 //=============================================================================
 /// Determine the size of the matrix blocks and setup the
 /// lookup schemes relating the global degrees of freedom with
 /// their "blocks" and their indices (row/column numbers) in those
 /// blocks.\n
 /// The distributions of the preconditioner and the blocks are
 /// automatically specified (and assumed to be uniform) at this
 /// stage.\n
 /// This method should be used if each any block contains more than one
 /// type of DOF. The argument vector dof_to_block_map should be of length
 /// ndof. Each element should contain an integer indicating the block number
 /// corresponding to that type of DOF.
 //=============================================================================
 template<typename MATRIX>
  void BlockPreconditioner<MATRIX>::block_setup(Problem* problem_pt, 
                                                DoubleMatrixBase* matrix_pt,
                                                Vector<unsigned> 
                                                dof_to_block_map)
  {
   // clear the memory
   this->clean_memory();

   // store the problem pointer
   Problem_pt = problem_pt;

   // get my_rank and nproc
#ifdef OOMPH_HAS_MPI
   unsigned my_rank = problem_pt->communicator_pt()->my_rank();
#endif
   unsigned nproc = problem_pt->communicator_pt()->nproc();

   // if this preconditioner is the master preconditioner then we need
   // to assemble the vectors : Dof_number
   //                           Index_in_dof_block
   if (Master_block_preconditioner_pt == 0)
    {

     // setup the distribution of this preconditioner
     // assumed to be the same as the matrix if the matrix is distribuatable
     bool distributed = false;
     if (dynamic_cast<DistributableLinearAlgebraObject*>(matrix_pt))
      {
       if (nproc > 1)
        {
         distributed = 
          dynamic_cast<DistributableLinearAlgebraObject*>
          (matrix_pt)->distribution_pt()->distributed();
        }
      }
     Distribution_pt->rebuild(problem_pt->communicator_pt(),
                              matrix_pt->nrow(),distributed); 
     Nrow = matrix_pt->nrow();

     // boolean to indicate whether the matrix is acutally distributed
     // ie distributed and on more than one processor
     bool matrix_distributed = false;
     if (Distribution_pt->distributed() && 
         Distribution_pt->communicator_pt()->nproc() > 1)
      {
       matrix_distributed = true;
      }

     // determine the global rows for which we should store the 
     // Dof_index and Index_in_dof_block for
     int min_matrix_index = 0;
     int max_matrix_index = matrix_pt->nrow()-1;
     if (matrix_distributed)
      {
       // determine the minimum and maximum global column indices on the 
       // processor
       CRDoubleMatrix* cr_matrix_pt = dynamic_cast<CRDoubleMatrix*>(matrix_pt);
       if (cr_matrix_pt)
        {
         min_matrix_index = cr_matrix_pt->first_row();
         max_matrix_index = min_matrix_index + cr_matrix_pt->nrow_local() - 1;
         unsigned nnz = cr_matrix_pt->nnz();
         int* column_index = cr_matrix_pt->column_index();
         for (unsigned i = 0; i < nnz; i++)
          {
           min_matrix_index = std::min(min_matrix_index,column_index[i]);
           max_matrix_index = std::max(max_matrix_index,column_index[i]);
          }
        }
       else
        {
         std::ostringstream error_message;
         error_message << "Block setup for distributed matrices only works "
                       << "for CRDoubleMatrices";
         throw OomphLibError(error_message.str(),
                             "BlockPreconditioner::block_setup(...)",
                             OOMPH_EXCEPTION_LOCATION);         
        }
      }

     // number of rows to be stored
     unsigned nlookup_rows = max_matrix_index - min_matrix_index + 1;

     // update the member data
     Nglobal_indices = nlookup_rows;
     Min_global_index = min_matrix_index;

     // resize the storage
     Dof_number.resize(nlookup_rows);
     Index_in_dof_block.resize(nlookup_rows);

     // How many meshes to we have?
     unsigned n_mesh=Mesh_pt.size();
    
     // Set Mesh_pt to the Problem' Mesh if nobody has made any
     // other assigment
     if (n_mesh==0)
      {
       Mesh_pt.push_back(problem_pt->mesh_pt());
       n_mesh=1;
      }
     else if ((n_mesh == 1)&&(Mesh_pt[0]==0))
      {
       Mesh_pt[0] = problem_pt->mesh_pt();
      }

     // zero the number of dof types
     Ndof_types = 0;

#ifdef PARANOID
     for (unsigned i=0;i<n_mesh;i++)
      {
       if (Mesh_pt[i]==0)
        {
         std::ostringstream error_message;
         error_message << "Error: Mesh_pt["<< i << "]=0 \n";
         throw OomphLibWarning(error_message.str(),
                               "BlockPreconditioner::block_setup",
                               OOMPH_EXCEPTION_LOCATION);
        }
      }
     // Vector to keep track of previously assigned block numbers
     // to check consistency between multple assignements.
     Vector<int> previously_assigned_block_number(Nglobal_indices,
                                                  Data::Is_unclassified);
#endif

     // determine whether the problem is distribution
     bool problem_distributed = false;
#ifdef OOMPH_HAS_MPI
     // the problem method distributed() is only accessible with MPI
     problem_distributed = problem_pt->distributed();

     // storage for the rows required by each processor
     // required_rows(p,0) is the minimum global index required by proc p
     //           ...(p,1) is the maximum global index required by proc p
     DenseMatrix<unsigned> required_rows(nproc,2);

     // populate required rows
     // every processor tells every other processor which rows
     // it is interested in
     unsigned* send_req_rows = new unsigned[2];
     send_req_rows[0] = min_matrix_index;
     send_req_rows[1] = max_matrix_index;
     unsigned* recv_req_rows = new unsigned[2*nproc];
     MPI_Allgather(send_req_rows,2,MPI_UNSIGNED,
                   recv_req_rows,2,MPI_UNSIGNED,
                   problem_pt->communicator_pt()->mpi_comm());
     delete[] send_req_rows;
     for (unsigned p = 0; p < nproc; p++)
      {
       required_rows(p,0) = recv_req_rows[2*p];
       required_rows(p,1) = recv_req_rows[2*p+1];
      }
     delete[] recv_req_rows;
#endif

     // if the problem is not distributed
     if (!problem_distributed)
      {

       // Offset for the block type in the overall system.
       // Different meshes contain different block-preconditionable
       // elements -- their blocks are added one after the other...
       unsigned dof_offset=0;

       // Loop over all meshes
       for (unsigned m=0;m<n_mesh;m++)
        {
         // Number of elements in this mesh
         unsigned n_element = this->Mesh_pt[m]->nelement();

         // Find the number of block types that the elements in this mesh
         // are in charge of
         unsigned ndof_in_element=
          this->Mesh_pt[m]->element_pt(0)->ndof_types();
         Ndof_types += ndof_in_element;

         // Loop over all elements
         for (unsigned e=0;e<n_element;e++)
          {

           // List containing pairs of global equation number and
           // dof number for each global dof in an element
           std::list<std::pair<unsigned long,unsigned> > dof_lookup_list;

           // Get list of blocks associated with the element's global unknowns
           this->Mesh_pt[m]->element_pt(e)->
            get_dof_numbers_for_unknowns(dof_lookup_list);

           // Loop over all entries in the list
           // and store the block number
           typedef std::list<std::pair<unsigned long,unsigned> >::iterator IT;
           for (IT it=dof_lookup_list.begin();
                it!=dof_lookup_list.end();it++)
            {
             unsigned long global_dof = it->first;
             if (global_dof >= unsigned(min_matrix_index) &&
                 global_dof <= unsigned(max_matrix_index))
              {
               unsigned dof_number = (it->second)+dof_offset;
               Dof_number[global_dof-min_matrix_index] 
                = dof_number;

#ifdef PARANOID
               // Check consistency of block numbers if assigned multiple times
               if (previously_assigned_block_number[global_dof-
                                                    min_matrix_index]<0)
                {
                 previously_assigned_block_number[global_dof-min_matrix_index]
                  =dof_number;
                }
               else
                {
                 if (previously_assigned_block_number[global_dof
                                                      -min_matrix_index]!=
                     int(dof_number))
                  {
                   std::ostringstream error_message;
                   error_message 
                    << "Inconsitency in assigment of block numbers\n"
                    << "Global dof " <<  global_dof
                    << "was previously assigned to block "
                    << previously_assigned_block_number[global_dof-
                                                        min_matrix_index]
                    << "\nNow it's been reassigned to block "
                    << dof_number << "\n";
                   throw OomphLibWarning(error_message.str(),
                                         "BlockPreconditioner::nblock_types()",
                                         OOMPH_EXCEPTION_LOCATION);
                  }
                }
#endif
              }
            }
          }

         // About to do the next mesh which contains block preconditionable
         // elements of a different type; all the dofs that these elements are
         // "in charge of" differ from the ones considered so far.
         // Bump up the block counter to make sure we're not overwriting
         // anything here
         dof_offset+=ndof_in_element;
        }

#ifdef PARANOID
       // check that every DOF number has been allocated
       bool success = true;
       for (unsigned i = 0; i < nlookup_rows; i++)
        {
         if (previously_assigned_block_number[i] < 0)
          {
           success = false;
          }
        }
       if (!success)
        {
         std::ostringstream error_message;
         error_message << "Not all degrees of freedom have had DOF type "
                       << "numbers allocated";
         throw OomphLibError(error_message.str(),
                             "BlockPreconditioner::block_setup(...)",
                             OOMPH_EXCEPTION_LOCATION);
        }
#endif
      }
     // else the problem is distributed
     else
      {
#ifdef OOMPH_HAS_MPI

       // Offset for the block type in the overall system.
       // Different meshes contain different block-preconditionable
       // elements -- their blocks are added one after the other...
       unsigned dof_offset=0;

       // the set of global degrees of freedom and their corresponding dof 
       // number on this processor
       // use set because the elements are ordered and unqiue
       std::set<std::pair<unsigned long,unsigned> > my_dof_set;

       // Loop over all meshes
       for (unsigned m=0;m<n_mesh;m++)
        {
         // Number of elements in this mesh
         unsigned n_element = this->Mesh_pt[m]->nelement();

         // Find the number of block types that the elements in this mesh
         // are in charge of
         unsigned ndof_in_element=
          this->Mesh_pt[m]->element_pt(0)->ndof_types();
         Ndof_types += ndof_in_element;

         // Loop over all elements
         for (unsigned e=0;e<n_element;e++)
          {

           // if the element is not a halo element
           if (!this->Mesh_pt[m]->element_pt(e)->is_halo())
            {
             // List containing pairs of global equation number and
             // dof number for each global dof in an element
             std::list<std::pair<unsigned long,unsigned> > dof_lookup_list;

             // Get list of blocks associated with the element's global
             // unknowns
             this->Mesh_pt[m]->element_pt(e)->
              get_dof_numbers_for_unknowns(dof_lookup_list);

             // update the block numbers
             typedef 
              std::list<std::pair<unsigned long,unsigned> >::iterator IT;
             for (IT it=dof_lookup_list.begin();
                  it!=dof_lookup_list.end();it++)
              {
               it->second = (it->second)+dof_offset;
              }

             // and insert them into our set
             my_dof_set.insert(dof_lookup_list.begin(),dof_lookup_list.end());
            }
          }

         // About to do the next mesh which contains block preconditionable
         // elements of a different type; all the dofs that these elements are
         // "in charge of" differ from the ones considered so far.
         // Bump up the block counter to make sure we're not overwriting
         // anything here
         dof_offset+=ndof_in_element;
        }

       // next copy the set of my dofs to two vectors to send
       unsigned my_ndof = my_dof_set.size();
       unsigned long* my_global_dofs = new unsigned long[my_ndof];
       unsigned* my_dof_numbers = new unsigned[my_ndof];
       typedef 
        std::set<std::pair<unsigned long,unsigned> >::iterator IT;       
       unsigned pt = 0;
       for (IT it = my_dof_set.begin(); it != my_dof_set.end(); it++)
        {
         my_global_dofs[pt] = it->first;
         my_dof_numbers[pt] = it->second;
         pt++;
        }

       // and then clear the set
       my_dof_set.clear();

       // count up how many DOFs need to be sent to each processor
       int* first_dof_to_send = new int[nproc];
       int* ndof_to_send = new int[nproc];
       for (unsigned p = 0; p < nproc; p++)
        {
         first_dof_to_send[p] = 0;
         ndof_to_send[p] = 0;
         unsigned ptr = 0;
         while (my_global_dofs[ptr] < required_rows(p,0) && ptr < my_ndof)
          {
           ptr++;
          }
         first_dof_to_send[p] = ptr;
         while (my_global_dofs[ptr] <= required_rows(p,1) && ptr < my_ndof)
          {
           ndof_to_send[p]++;
           ptr++;
          }
        }

       // next communicate to each processor how many dofs it expects to recv
       int* ndof_to_recv = new int[nproc];
       MPI_Alltoall(ndof_to_send,1,MPI_INT,ndof_to_recv,1,MPI_INT,
                    problem_pt->communicator_pt()->mpi_comm());

       // the base displacements for the sends
       MPI_Aint base_displacement;
       MPI_Address(my_global_dofs,&base_displacement);

#ifdef PARANOID
       // storage for paranoid check to ensure that every row as been
       // imported
       std::vector<bool> dof_recv(nlookup_rows,false);
#endif

       // next send and recv
       Vector<MPI_Request> send_requests;
       Vector<MPI_Request> recv_requests;
       Vector<unsigned long*> global_dofs_recv(nproc,0);
       Vector<unsigned*> dof_numbers_recv(nproc,0);
       Vector<unsigned> proc;
       for (unsigned p = 0; p < nproc; p++)
        {
         if (p != my_rank)
          {
           // send
           if (ndof_to_send[p] > 0)
            {
             // the datatypes, displacements, lengths for the two datatypes
             MPI_Datatype types[2];
             MPI_Aint displacements[2];
             int lengths[2];

             // my global dofs
             MPI_Type_contiguous(ndof_to_send[p],MPI_UNSIGNED_LONG,&types[0]);
             MPI_Type_commit(&types[0]);
             MPI_Address(my_global_dofs + first_dof_to_send[p],
                         &displacements[0]);
             displacements[0] -= base_displacement;
             lengths[0] = 1;

             // my dof numbers
             MPI_Type_contiguous(ndof_to_send[p],MPI_UNSIGNED,&types[1]);
             MPI_Type_commit(&types[1]);
             MPI_Address(my_dof_numbers + first_dof_to_send[p],
                         &displacements[1]);
             displacements[1] -= base_displacement;
             lengths[1] = 1;           

             // build the final type
             MPI_Datatype send_type;
             MPI_Type_struct(2,lengths,displacements,types,&send_type);
             MPI_Type_commit(&send_type);
             MPI_Type_free(&types[0]);
             MPI_Type_free(&types[1]);

             // and send
             MPI_Request req;
             MPI_Isend(my_global_dofs,1,send_type,p,2,
                       problem_pt->communicator_pt()->mpi_comm(),&req);
             send_requests.push_back(req);
             MPI_Type_free(&send_type);
            }

           // and recv
           if (ndof_to_recv[p] > 0)
            {
             // resize the storage
             global_dofs_recv[p] = new unsigned long[ndof_to_recv[p]];
             dof_numbers_recv[p] = new unsigned[ndof_to_recv[p]];
             proc.push_back(p);

             // the datatypes, displacements, lengths for the two datatypes
             MPI_Datatype types[2];
             MPI_Aint displacements[2];
             int lengths[2];

             // my global dofs
             MPI_Type_contiguous(ndof_to_recv[p],MPI_UNSIGNED_LONG,&types[0]);
             MPI_Type_commit(&types[0]);
             MPI_Address(global_dofs_recv[p],&displacements[0]);
             displacements[0] -= base_displacement;
             lengths[0] = 1;

             // my dof numbers
             MPI_Type_contiguous(ndof_to_recv[p],MPI_UNSIGNED,&types[1]);
             MPI_Type_commit(&types[1]);
             MPI_Address(dof_numbers_recv[p],&displacements[1]);
             displacements[1] -= base_displacement;
             lengths[1] = 1;           

             // build the final type
             MPI_Datatype recv_type;
             MPI_Type_struct(2,lengths,displacements,types,&recv_type);
             MPI_Type_commit(&recv_type);
             MPI_Type_free(&types[0]);
             MPI_Type_free(&types[1]);

             // and recv
             MPI_Request req;
             MPI_Irecv(my_global_dofs,1,recv_type,p,2,
                       problem_pt->communicator_pt()->mpi_comm(),&req);
             recv_requests.push_back(req);
             MPI_Type_free(&recv_type);
            }
          }
         // send to self
         else
          {
           unsigned u = first_dof_to_send[p] + ndof_to_recv[p];
           for (unsigned i = first_dof_to_send[p]; i < u; i++)
            {
#ifdef PARANOID
             // paranoid check
             // check that if row has been imported the block number is the 
             // same
             if (dof_recv[my_global_dofs[i]-min_matrix_index])
              {
               if (Dof_number[my_global_dofs[i]-min_matrix_index] 
                   != my_dof_numbers[i])
                {
                 std::ostringstream error_message;
                 error_message 
                  << "Inconsitency in assigment of block numbers\n"
                  << "Global dof " <<  my_global_dofs[i]
                  << "was previously assigned to block " 
                  <<  Dof_number[my_global_dofs[i]-min_matrix_index]
                  << "\nNow it's been reassigned to block "
                  << my_dof_numbers[i] << "\n";
                 throw OomphLibWarning(error_message.str(),
                                       "BlockPreconditioner::block_setup(...)",
                                       OOMPH_EXCEPTION_LOCATION);
                }
              }
             // indicate that this dof has ben recv
             dof_recv[my_global_dofs[i]-min_matrix_index] = true;
#endif
             Dof_number[my_global_dofs[i]-min_matrix_index] = 
              my_dof_numbers[i];   
            }
          }
        }

       // recv and store the data
       unsigned c_recv = recv_requests.size();
       while (c_recv > 0)
        {

         // wait for any communication to finish
         int req_number;
         MPI_Waitany(c_recv,&recv_requests[0],&req_number,MPI_STATUS_IGNORE);
         recv_requests.erase(recv_requests.begin()+req_number);
         c_recv--;

         // determine the source processor
         unsigned p = proc[req_number];
         proc.erase(proc.begin()+req_number);

         // import the data
         for (int i  = 0; i < ndof_to_recv[p]; i++)
          {
#ifdef PARANOID
           // paranoid check
           // check that if row has been imported the block number is the same
           if (dof_recv[global_dofs_recv[p][i]-min_matrix_index])
            {
             if (Dof_number[global_dofs_recv[p][i]-min_matrix_index] 
                 != dof_numbers_recv[p][i])
              {
               std::ostringstream error_message;
               error_message << "Inconsitency in assignment of block numbers\n"
                             << "Global dof " 
                             <<  global_dofs_recv[p][i]
                             << " was previously assigned to block " 
                             <<  Dof_number[global_dofs_recv[p][i]
                                            -min_matrix_index]
                             << "\nNow it's been reassigned to block "
                             << dof_numbers_recv[p][i] << "\n";
               throw OomphLibWarning(error_message.str(),
                                     "BlockPreconditioner::block_setup(...)",
                                     OOMPH_EXCEPTION_LOCATION);
              }
            }
           // indicate that this dof has ben recv
           dof_recv[global_dofs_recv[p][i]-min_matrix_index] = true;
#endif
           Dof_number[global_dofs_recv[p][i]-min_matrix_index] 
            = dof_numbers_recv[p][i];
          }

         // delete the data
         delete[] global_dofs_recv[p];
         delete[] dof_numbers_recv[p];
        }

       // finally wait for the send requests to complete as we are leaving
       // an MPI block of code
       unsigned csr = send_requests.size();
       if (csr)
        {
         MPI_Waitall(csr,&send_requests[0],MPI_STATUS_IGNORE);
        }

       // clean up
       delete[] ndof_to_send;
       delete[] first_dof_to_send;
       delete[] ndof_to_recv;
       delete[] my_global_dofs;
       delete[] my_dof_numbers;
#ifdef PARANOID
       unsigned all_recv = true;
       for (unsigned i = 0; i < nlookup_rows; i++)
        {
         if (!dof_recv[i])
          {
           all_recv = false;
          }
        }
       if (!all_recv)
        {
         std::ostringstream error_message;
         error_message << "Not all the DOF numbers required were recieved";
         throw OomphLibError(error_message.str(),
                             "BlockPreconditioner::block_setup()",
                             OOMPH_EXCEPTION_LOCATION);
        }
#endif


#else
       std::ostringstream error_message;
       error_message << "The problem appears to be distributed, MPI is required";
       throw OomphLibError(error_message.str(),
                           "BlockPreconditioner::block_setup()",
                           OOMPH_EXCEPTION_LOCATION);
#endif
      }

     // for every global degree of freedom required by this processor we now 
     // have the corresponding dof number

     // clear the Ndof_in_dof_block storage
     Dof_dimension.resize(Ndof_types);
     Dof_dimension.initialise(0);

     // first consider a non distributed matrix
     if (!matrix_distributed)
      {

       // set the Index_in_dof_block
       unsigned nrow  = Distribution_pt->nrow();
       Index_in_dof_block.resize(nrow);
       Index_in_dof_block.initialise(0);
       for (unsigned i = 0; i < nrow; i++)
        {
         Index_in_dof_block[i] = Dof_dimension[Dof_number[i]];
         Dof_dimension[Dof_number[i]]++;
        }
      }

     // next a distributed matrix
     else
      {
#ifdef OOMPH_HAS_MPI

       // first compute how many instances of each dof are on this 
       // processor
       unsigned* my_nrows_in_dof_block = new unsigned[Ndof_types];
       for (unsigned i = 0; i < Ndof_types; i++)
        {
         my_nrows_in_dof_block[i] = 0;
        }
       unsigned nrow_local = Distribution_pt->nrow_local();
       unsigned first_row = Distribution_pt->first_row();
       for (unsigned i = 0; i < nrow_local; i++)
        {
         my_nrows_in_dof_block[Dof_number[first_row+i-Min_global_index]]++;
        }

       // next share the data
       unsigned* nrow_in_dof_block_recv = new unsigned[Ndof_types*nproc];
       MPI_Allgather(my_nrows_in_dof_block,Ndof_types,MPI_UNSIGNED,
                     nrow_in_dof_block_recv,Ndof_types,MPI_UNSIGNED,
                     problem_pt->communicator_pt()->mpi_comm());
       delete[] my_nrows_in_dof_block;

       // compute my first dof index and Nrows_in_dof_block
       Vector<unsigned> my_first_dof_index(Ndof_types,0);
       for (unsigned i = 0; i < Ndof_types; i++)
        {
         for (unsigned p = 0; p < my_rank; p++)
          {
           my_first_dof_index[i] += nrow_in_dof_block_recv[p*Ndof_types + i];
          }
         Dof_dimension[i] = my_first_dof_index[i];
         for (unsigned p = my_rank; p < nproc; p++)
          {
           Dof_dimension[i] += nrow_in_dof_block_recv[p*Ndof_types + i];
          }
        }
       delete[] nrow_in_dof_block_recv;

       // next compute Index in dof block
       Index_in_dof_block.resize(nlookup_rows);
       Index_in_dof_block.initialise(0);
       Vector<unsigned> dof_counter(Ndof_types,0);
       for (unsigned i = 0; i < nrow_local; i++)
        {
         unsigned j = i + first_row - min_matrix_index;
         Index_in_dof_block[j] = my_first_dof_index[Dof_number[j]] + 
          dof_counter[Dof_number[j]];
         dof_counter[Dof_number[j]]++;
        }

       // next we send and recv in the Index_in_dof_block with the 
       // other processors
       // so far we have only computed the local Index_in_dof_block
       Vector<MPI_Request> requests;
       for (unsigned p = 0; p < nproc; p++)
        {
         if (p != my_rank)
          {
           // first the sends
           unsigned first_row_send = 0;
           unsigned nrow_send = 0;
           if ((required_rows(p,0) < (Distribution_pt->first_row() +
                                      Distribution_pt->nrow_local())) &&
               (Distribution_pt->first_row() <= required_rows(p,1)))
            {
             first_row_send = std::max(Distribution_pt->first_row(),
                                       required_rows(p,0));
             nrow_send = std::min((Distribution_pt->first_row() + 
                                   Distribution_pt->nrow_local()),
                                  required_rows(p,1)+1)
              - first_row_send;
            }
           MPI_Request sreq;
           MPI_Isend(&Index_in_dof_block[first_row_send-Min_global_index],
                     nrow_send,MPI_UNSIGNED,p,3,
                     problem_pt->communicator_pt()->mpi_comm(),&sreq);
           requests.push_back(sreq);

           // and then the recvs
           unsigned first_row_recv = 0;
           unsigned nrow_recv = 0;
           if ((required_rows(my_rank,0) < (Distribution_pt->first_row(p) +
                                            Distribution_pt->nrow_local(p)))
               && (Distribution_pt->first_row(p) <= required_rows(my_rank,1)))
            {
             first_row_recv = std::max(Distribution_pt->first_row(p),
                                       required_rows(my_rank,0));
             nrow_recv = std::min(Distribution_pt->first_row(p) + 
                                  Distribution_pt->nrow_local(p),
                                  required_rows(my_rank,1)+1) - first_row_recv;
            }
           MPI_Request rreq;
           MPI_Irecv(&Index_in_dof_block[first_row_recv-Min_global_index],
                     nrow_recv,MPI_UNSIGNED,p,3,
                     problem_pt->communicator_pt()->mpi_comm(),&rreq);
           requests.push_back(rreq);
          }
        }

       // and then wait
       unsigned nreq = requests.size();
       if (nreq)
        {
         MPI_Waitall(nreq,&requests[0],MPI_STATUS_IGNORE);
        }
#endif
      }
    }

   // compute the number of rows in each block

#ifdef PARANOID
   //check the vector is the correct length
   if (dof_to_block_map.size() != Ndof_types) 
    {
     std::ostringstream error_message;
     error_message  
      << "The dof_to_block_map vector must be of size Ndof_type";
     throw OomphLibWarning(
      error_message.str(),
      "BlockPreconditioner::block_setup(...)",
      OOMPH_EXCEPTION_LOCATION);
    }
#endif

   // find the maximum block number
   unsigned max_block_number = 0;
   for (unsigned i = 0; i < Ndof_types; i++)
    {
     if (dof_to_block_map[i] > max_block_number)
      {
       max_block_number = dof_to_block_map[i];
      }
    }

   // resize the storage the the block to dof map
   Block_number_to_dof_number_lookup.clear();
   Block_number_to_dof_number_lookup.resize(max_block_number+1);
   Ndof_in_block.clear();
   Ndof_in_block.resize(max_block_number+1);

   // resize storage
   Dof_number_to_block_number_lookup.resize(Ndof_types);

   // build the storage for the two maps (block to dof) and (dof to block)
   for (unsigned i = 0; i < Ndof_types; i++)
    {
     Dof_number_to_block_number_lookup[i] = dof_to_block_map[i];
     Block_number_to_dof_number_lookup[dof_to_block_map[i]].push_back(i);
     Ndof_in_block[dof_to_block_map[i]]++;
    }

#ifdef PARANOID
   // paranoid check that every block number has at least one DOF associated
   // with it
   for (unsigned i = 0; i < max_block_number+1; i++)
    {
     if (Block_number_to_dof_number_lookup[i].size() == 0)
      {
       std::ostringstream error_message;
       error_message  << "block number " << i 
                      << " does not have any DOFs associated with it";
       throw OomphLibWarning(
        error_message.str(),
        "BlockPreconditioner::block_setup(...)",
        OOMPH_EXCEPTION_LOCATION);
      }
    }
#endif

   // update the number of blocks types
   Nblock_types = max_block_number+1;

   // distributed or not depends on if we have more than one processor
   bool distributed = this->master_distribution_pt()->distributed();

   // create the new block distributions
   Block_distribution_pt.resize(Nblock_types);
   for (unsigned i = 0; i < Nblock_types; i++)
    {
     unsigned block_dim = 0;
     for (unsigned j = 0; j < Ndof_in_block[i]; j++)
      {
       block_dim += 
        dof_block_dimension(Block_number_to_dof_number_lookup[i][j]);
      }
     Block_distribution_pt[i] = new 
      LinearAlgebraDistribution(Problem_pt->communicator_pt(),
                                block_dim,distributed);
    }

   // create the distribution of this preconditioner
   if (Master_block_preconditioner_pt != 0)
    {
     Distribution_pt->rebuild(Problem_pt->communicator_pt(),Nrow,
                              distributed);
    }

   // next we assemble the lookup schemes for the rows
   // if the matrix is not distributed then we assemble Global_index
   // if the matrix is distributed then Rows_to_send_..., Rows_to_recv_... etc
   if (!distributed)
    {
     // resize the storage
     Global_index.resize(Nblock_types);
     for (unsigned b = 0; b < Nblock_types; b++)
      {
       Global_index[b].resize(Block_distribution_pt[b]->nrow());
      }

     // compute 
     unsigned nrow = this->master_nrow();
     for (unsigned i = 0; i < nrow; i++)
      {
       // the dof type number
       int dof_number = this->dof_number(i);
       if (dof_number >= 0)
        {

         // the block number
         unsigned block_number = Dof_number_to_block_number_lookup[dof_number];

         // compute the index in the block
         unsigned index_in_block = 0;
         unsigned ptr = 0;
         while (int(Block_number_to_dof_number_lookup[block_number][ptr]) 
                != dof_number)
          {
           index_in_block +=
            dof_block_dimension(Block_number_to_dof_number_lookup[block_number]
                                [ptr]);
           ptr++;
          }
         index_in_block += index_in_dof(i);
         Global_index[block_number][index_in_block] = i;
        }
      }
    }
   // otherwise the matrix is distributed
   else
    {
#ifdef OOMPH_HAS_MPI

     // the pointer to the master distribution
     const LinearAlgebraDistribution* master_distribution_pt = 
      this->master_distribution_pt();

     // resize the nrows... storage
     Nrows_to_send_for_get_block.resize(Nblock_types,nproc);
     Nrows_to_send_for_get_block.initialise(0);
     Nrows_to_send_for_get_ordered.resize(nproc);
     Nrows_to_send_for_get_ordered.initialise(0);

     // loop over my rows
     unsigned nrow_local = master_distribution_pt->nrow_local();
     unsigned first_row = master_distribution_pt->first_row();
     for (unsigned i = 0; i < nrow_local; i++)
      {

       // the block number
       int b = this->block_number(first_row + i);

       // check that the DOF i is associated with this preconditioner
       if (b >= 0)
        {
         // the block index
         unsigned j = this->index_in_block(first_row + i);

         // the processor this row will be sent to
         unsigned block_p = 0;
         while(!(Block_distribution_pt[b]->first_row(block_p) <= j &&
                 (Block_distribution_pt[b]->first_row(block_p) +
                  Block_distribution_pt[b]->nrow_local(block_p) > j)))
          {
           block_p++;
          }

         // and increment the counter
         Nrows_to_send_for_get_block(b,block_p)++;

         // next compute the index a block ordered matrix / vector
         unsigned l = j;
         for (int k = 0; k < b; k++)
          {
           l += Block_distribution_pt[k]->nrow();
          }

         // and next the processor this global row should be sent to
         unsigned ordered_p = 0;
         while(!(Distribution_pt->first_row(ordered_p) <= l &&
                 (Distribution_pt->first_row(ordered_p) +
                  Distribution_pt->nrow_local(ordered_p) > l)))
          {
           ordered_p++;
          }

         // and increment the counter
         Nrows_to_send_for_get_ordered[ordered_p]++;
        }
      }

     // resize the storage for Nrows_to_recv
     Nrows_to_recv_for_get_block.resize(Nblock_types,nproc);
     Nrows_to_recv_for_get_block.initialise(0);
     Nrows_to_recv_for_get_ordered.resize(nproc);
     Nrows_to_recv_for_get_ordered.initialise(0);

     // next we send the number of rows that will be sent by this processor
     Vector<unsigned*> nrows_to_send(nproc,0);
     Vector<unsigned*> nrows_to_recv(nproc,0);
     Vector<MPI_Request> send_requests_nrow;
     Vector<MPI_Request> recv_requests_nrow;
     Vector<unsigned> proc;
     for (unsigned p = 0; p < nproc; p++)
      {
       if (p != my_rank)
        {
         // send 
         proc.push_back(p);
         nrows_to_send[p] = new unsigned[Nblock_types + 1];
         for (unsigned b = 0; b < Nblock_types; b++)
          {
           nrows_to_send[p][b] = 
            Nrows_to_send_for_get_block(b,p);
          }
         nrows_to_send[p][Nblock_types] = 
          Nrows_to_send_for_get_ordered[p];
         MPI_Request s_req;
         MPI_Isend(nrows_to_send[p],Nblock_types+1,MPI_UNSIGNED,p,3,
                   problem_pt->communicator_pt()->mpi_comm(),&s_req);
         send_requests_nrow.push_back(s_req);

         // recv
         nrows_to_recv[p] = new unsigned[Nblock_types + 1];
         MPI_Request r_req;
         MPI_Irecv(nrows_to_recv[p],Nblock_types+1,MPI_UNSIGNED,p,3,
                   problem_pt->communicator_pt()->mpi_comm(),&r_req);
         recv_requests_nrow.push_back(r_req);
        }
       // send to self
       else
        {
         for (unsigned b = 0; b < Nblock_types; b++)
          {
           Nrows_to_recv_for_get_block(b,p) =
            Nrows_to_send_for_get_block(b,p);
          }         
         Nrows_to_recv_for_get_ordered[p] = Nrows_to_send_for_get_ordered[p];
        }
      }

     // create some temporary storage for the global row indices that will
     // be received from another processor. 
     DenseMatrix<int*> block_rows_to_send(Nblock_types,nproc,0);
     Vector<int*> ordered_rows_to_send(nproc,0);

     // resize the rows... storage
     Rows_to_send_for_get_block.resize(Nblock_types,nproc);
     Rows_to_send_for_get_block.initialise(0);
     Rows_to_send_for_get_ordered.resize(nproc);
     Rows_to_send_for_get_ordered.initialise(0);
     Rows_to_recv_for_get_block.resize(Nblock_types,nproc);
     Rows_to_recv_for_get_block.initialise(0);
     Rows_to_recv_for_get_ordered.resize(nproc);
     Rows_to_recv_for_get_ordered.initialise(0);

     // resize the storage
     for (unsigned p = 0; p < nproc; p++)
      {
       for (unsigned b = 0; b < Nblock_types; b++)
        {
         Rows_to_send_for_get_block(b,p)
          = new int[Nrows_to_send_for_get_block(b,p)];
         if (p != my_rank)
          {
           block_rows_to_send(b,p)
            = new int[Nrows_to_send_for_get_block(b,p)];
          }
         else
          {
           Rows_to_recv_for_get_block(b,p)
            = new int[Nrows_to_send_for_get_block(b,p)];
          }
        }
       Rows_to_send_for_get_ordered[p] 
        = new int [Nrows_to_send_for_get_ordered[p]];
       if (p != my_rank)
        {
         ordered_rows_to_send[p] 
          = new int [Nrows_to_send_for_get_ordered[p]];
        }
       else
        {
         Rows_to_recv_for_get_ordered[p]
          = new int [Nrows_to_send_for_get_ordered[p]];
        }
      }

     // loop over my rows to allocate the nrows
     DenseMatrix<unsigned> ptr_block(Nblock_types,nproc,0);
     Vector<unsigned> ptr_ordered(nproc,0);
     for (unsigned i = 0; i < nrow_local; i++)
      {
       // the block number
       int b = this->block_number(first_row + i);

       // check that the DOF i is associated with this preconditioner
       if (b >= 0)
        {       

         // the block index
         unsigned j = this->index_in_block(first_row + i);

         // the processor this row will be sent to
         unsigned block_p = 0;
         while(!(Block_distribution_pt[b]->first_row(block_p) <= j &&
                 (Block_distribution_pt[b]->first_row(block_p) +
                  Block_distribution_pt[b]->nrow_local(block_p) > j)))
          {
           block_p++;
          }

         // and store the row
         Rows_to_send_for_get_block(b,block_p)[ptr_block(b,block_p)] = i;
         if (block_p != my_rank)
          {
           block_rows_to_send(b,block_p)[ptr_block(b,block_p)] 
            = j - Block_distribution_pt[b]->first_row(block_p);
          }
         else
          {
           Rows_to_recv_for_get_block(b,block_p)[ptr_block(b,block_p)] 
            = j - Block_distribution_pt[b]->first_row(block_p);
          }
         ptr_block(b,block_p)++;

         // next compute the index a block ordered matrix / vector
         unsigned l = j;
         for (int k = 0; k < b; k++)
          {
           l += Block_distribution_pt[k]->nrow();
          }

         // and next the processor this global row should be sent to
         unsigned ordered_p = 0;
         while(!(Distribution_pt->first_row(ordered_p) <= l &&
                 (Distribution_pt->first_row(ordered_p) +
                  Distribution_pt->nrow_local(ordered_p) > l)))
          {
           ordered_p++;
          }

         // and store the row
         Rows_to_send_for_get_ordered[ordered_p][ptr_ordered[ordered_p]] = i;
         if (ordered_p != my_rank)
          {
           ordered_rows_to_send[ordered_p][ptr_ordered[ordered_p]] = 
            l - Distribution_pt->first_row(ordered_p);
          }
         else
          {
           Rows_to_recv_for_get_ordered[ordered_p][ptr_ordered[ordered_p]] = 
            l - Distribution_pt->first_row(ordered_p);
          }
         ptr_ordered[ordered_p]++;
        }
      }

     // next process the nrow recvs as they complete

     // recv and store the data
     unsigned c = recv_requests_nrow.size();
     while (c > 0)
      {

       // wait for any communication to finish
       int req_number;
       MPI_Waitany(c,&recv_requests_nrow[0],&req_number,MPI_STATUS_IGNORE);
       recv_requests_nrow.erase(recv_requests_nrow.begin()+req_number);
       c--;

       // determine the source processor
       unsigned p = proc[req_number];
       proc.erase(proc.begin()+req_number);

       // copy the data to its final storage
       for (unsigned b = 0; b < Nblock_types; b++)
        {
         Nrows_to_recv_for_get_block(b,p) = nrows_to_recv[p][b];
        }
       Nrows_to_recv_for_get_ordered[p] = nrows_to_recv[p][Nblock_types];

       // and clear
       delete[] nrows_to_recv[p];
      }

     // resize the storage for the incoming rows data
     for (unsigned p = 0; p < nproc; p++)
      {
       if (p != my_rank)
        {
         for (unsigned b = 0; b < Nblock_types; b++)
          {
           Rows_to_recv_for_get_block(b,p)
            = new int[Nrows_to_recv_for_get_block(b,p)];
          }
         Rows_to_recv_for_get_ordered[p] 
          = new int [Nrows_to_recv_for_get_ordered[p]];
        }
      }

     // compute the number of sends and recv from this processor
     // to each other processor
     Vector<unsigned> nsend_for_rows(nproc,0);
     Vector<unsigned> nrecv_for_rows(nproc,0);
     for (unsigned p = 0; p < nproc; p++)
      {
       if (p != my_rank)
        {
         for (unsigned b = 0; b < Nblock_types; b++)
          {
           if (Nrows_to_send_for_get_block(b,p) > 0)
            {
             nsend_for_rows[p]++;
            }
           if (Nrows_to_recv_for_get_block(b,p) > 0)
            {
             nrecv_for_rows[p]++;
            }
          }
         if (Nrows_to_send_for_get_ordered[p] > 0)
          {
           nsend_for_rows[p]++;
          }
         if (Nrows_to_recv_for_get_ordered[p] > 0)
          {
           nrecv_for_rows[p]++;
          }
        }
      }

     // finally post the sends and recvs
     MPI_Aint base_displacement;
     MPI_Address(matrix_pt,&base_displacement);
     Vector<MPI_Request> req_rows;
     for (unsigned p = 0; p < nproc; p++)
      {
       if (p != my_rank)
        {
         // send
         if (nsend_for_rows[p] > 0)
          {
           MPI_Datatype send_types[nsend_for_rows[p]];
           MPI_Aint send_displacements[nsend_for_rows[p]];
           int send_sz[nsend_for_rows[p]];
           unsigned send_ptr = 0;
           for (unsigned b = 0; b < Nblock_types; b++)
            {
             if (Nrows_to_send_for_get_block(b,p) > 0)
              {
               MPI_Type_contiguous(Nrows_to_send_for_get_block(b,p),
                                   MPI_INT,&send_types[send_ptr]);
               MPI_Type_commit(&send_types[send_ptr]);
               MPI_Address(block_rows_to_send(b,p),
                           &send_displacements[send_ptr]);
               send_displacements[send_ptr] -= base_displacement;
               send_sz[send_ptr] = 1;
               send_ptr++;
              }
            }
           if (Nrows_to_send_for_get_ordered[p] > 0)
            {
             MPI_Type_contiguous(Nrows_to_send_for_get_ordered[p],
                                 MPI_INT,&send_types[send_ptr]);
             MPI_Type_commit(&send_types[send_ptr]);
             MPI_Address(ordered_rows_to_send[p],
                         &send_displacements[send_ptr]);
             send_displacements[send_ptr] -= base_displacement;
             send_sz[send_ptr] = 1;
            }
           MPI_Datatype final_send_type;
           MPI_Type_struct(nsend_for_rows[p],send_sz,send_displacements,
                           send_types,&final_send_type);
           MPI_Type_commit(&final_send_type);
           for (unsigned i = 0; i < nsend_for_rows[p]; i++)
            {
             MPI_Type_free(&send_types[i]);
            }
           MPI_Request send_req;
           MPI_Isend(matrix_pt,1,final_send_type,p,4,
                     problem_pt->communicator_pt()->mpi_comm(),&send_req);
           req_rows.push_back(send_req);
           MPI_Type_free(&final_send_type);
          }

         // recv
         if (nrecv_for_rows[p] > 0)
          {
           MPI_Datatype recv_types[nrecv_for_rows[p]];
           MPI_Aint recv_displacements[nrecv_for_rows[p]];
           int recv_sz[nrecv_for_rows[p]];
           unsigned recv_ptr = 0;
           for (unsigned b = 0; b < Nblock_types; b++)
            {
             if (Nrows_to_recv_for_get_block(b,p) > 0)
              {
               MPI_Type_contiguous(Nrows_to_recv_for_get_block(b,p),
                                   MPI_INT,&recv_types[recv_ptr]);
               MPI_Type_commit(&recv_types[recv_ptr]);
               MPI_Address(Rows_to_recv_for_get_block(b,p),
                           &recv_displacements[recv_ptr]);
               recv_displacements[recv_ptr] -= base_displacement;
               recv_sz[recv_ptr] = 1;
               recv_ptr++;
              }
            }
           if (Nrows_to_recv_for_get_ordered[p] > 0)
            {
             MPI_Type_contiguous(Nrows_to_recv_for_get_ordered[p],
                                 MPI_INT,&recv_types[recv_ptr]);
             MPI_Type_commit(&recv_types[recv_ptr]);
             MPI_Address(Rows_to_recv_for_get_ordered[p],
                         &recv_displacements[recv_ptr]);
             recv_displacements[recv_ptr] -= base_displacement;
             recv_sz[recv_ptr] = 1;
            }
           MPI_Datatype final_recv_type;
           MPI_Type_struct(nrecv_for_rows[p],recv_sz,recv_displacements,
                           recv_types,&final_recv_type);
           MPI_Type_commit(&final_recv_type);
           for (unsigned i = 0; i < nrecv_for_rows[p]; i++)
            {
             MPI_Type_free(&recv_types[i]);
            }
           MPI_Request recv_req;
           MPI_Irecv(matrix_pt,1,final_recv_type,p,4,
                     problem_pt->communicator_pt()->mpi_comm(),&recv_req);
           req_rows.push_back(recv_req);
           MPI_Type_free(&final_recv_type);
          }
        }
      }

     // cleaning up Waitalls
     unsigned n_req_send_nrow = send_requests_nrow.size();
     if (n_req_send_nrow)
      {
       MPI_Waitall(n_req_send_nrow,&send_requests_nrow[0],MPI_STATUS_IGNORE);
      }
     for (unsigned p = 0; p < nproc; p++)
      {
       delete[] nrows_to_send[p];
      }
     unsigned n_req_rows = req_rows.size();
     if (n_req_rows)
      {
       MPI_Waitall(n_req_rows,&req_rows[0],MPI_STATUS_IGNORE);
      }
     for (unsigned p = 0; p < nproc; p++)
      {
       if (p!= my_rank)
        {
         for (unsigned b = 0; b < Nblock_types; b++)
          {
           if (Nrows_to_send_for_get_block(b,p) >= 0)
            {
             delete[] block_rows_to_send(b,p);
            }
          }
         if (Nrows_to_send_for_get_ordered[p] > 0)
          {
           delete[] ordered_rows_to_send[p];
          }
        }
      }
#endif
    }
  }




 //============================================================================
 /// Determine the size of the matrix blocks and setup the
 /// lookup schemes relating the global degrees of freedom with
 /// their "blocks" and their indices (row/column numbers) in those
 /// blocks.\n
 /// The distributions of the preconditioner and the blocks are
 /// automatically specified (and assumed to be uniform) at this
 /// stage.\n
 /// This method should be used if each DOF type corresponds to a 
 /// unique block type.
 //============================================================================
 template<typename MATRIX>
  void BlockPreconditioner<MATRIX>::block_setup(Problem* problem_pt, 
                                                DoubleMatrixBase* matrix_pt)
  {
   // get the number of DOF types
   unsigned ndof_types = 0;
   if (Master_block_preconditioner_pt == 0)
    {
     unsigned n_mesh = Mesh_pt.size();

     if (n_mesh != 0)
      {
       for (unsigned m = 0; m < n_mesh; m++)
        {
         ndof_types += Mesh_pt[m]->element_pt(0)->ndof_types();
        }
      }
     else
      {
       ndof_types = problem_pt->mesh_pt()->element_pt(0)->ndof_types();
      }
    }
   else
    {
     ndof_types = Ndof_types;
    }

   // build the dof to block map - assume that eac type of dof corresponds
   // to a different type of block
   Vector<unsigned> dof_to_block_lookup(ndof_types);
   for (unsigned i = 0; i < ndof_types; i++)
    {
     dof_to_block_lookup[i] = i;
    }

   // call the block setup method
   this->block_setup(problem_pt,matrix_pt,dof_to_block_lookup);
  }


 //============================================================================
 /// Get the block matrices required for the block preconditioner. 
 /// Takes the pointer to the original matrix and a matrix of bools
 /// that indicate if a specified sub-block is required
 /// for the preconditioning operation. Computes the
 /// required block matrices, and stores pointers to them
 /// in the matrix block_matrix_pt. If an entry in block_matrix_pt
 /// is equal to NULL that sub-block has not been requested and is
 /// therefore not available. 
 //============================================================================
 template<typename MATRIX> 
  void BlockPreconditioner<MATRIX>::get_blocks(
   MATRIX* matrix_pt,  DenseMatrix<bool>& required_blocks,
   DenseMatrix<MATRIX*>& block_matrix_pt)
  {

   // Cache number of block types
   const unsigned n_block_types=this->Nblock_types;

#ifdef PARANOID
   // if required blocks matrix is not the correct size then abort
   if ((required_blocks.nrow() != n_block_types) || 
       (required_blocks.ncol() != n_block_types))
    {

     std::ostringstream error_message;
     error_message << "The size of the matrix of bools required_blocks" 
                   << "(which indicates which blocks are required) is not the "
                   << "right size, required_blocks is " 
                   << required_blocks.ncol()
                   << " x " << required_blocks.nrow() << ", whereas it should "
                   << "be " << n_block_types << " x " << n_block_types;
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner<MATRIX>::get_blocks()",
                         OOMPH_EXCEPTION_LOCATION);
    }
   // if required blocks matrix is not the correct size then abort
   if ((block_matrix_pt.nrow() != n_block_types) ||
       (block_matrix_pt.ncol() != n_block_types))
    {
     std::ostringstream error_message;
     error_message << "The size of the block matrix pt is not the "
                   << "right size, block_matrix_pt is "
                   << block_matrix_pt.ncol()
                   << " x " << block_matrix_pt.nrow() << ", whereas it should "
                   << "be " << n_block_types << " x " << n_block_types;
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner<MATRIX>::get_blocks()",
                         OOMPH_EXCEPTION_LOCATION);
    }

#endif

   // loops over the blocks
   for (unsigned i = 0; i < n_block_types; i++)
    {

     for (unsigned j = 0; j < n_block_types; j++)
      {

       // if block(i,j) is required then
       if (required_blocks(i,j))
        {

         get_block(i,j,matrix_pt,block_matrix_pt(i,j));

        }
       // if the block is not required then set pointer to null
       else
        {
         block_matrix_pt(i,j) = 0;	
        }
      }
    }
  }

 //============================================================================
 /// \short Takes the naturally ordered vector and rearranges it into a vector
 /// of sub vectors corresponding to the blocks, so s[b][i] contains
 /// the i-th entry in the vector associated with block b.
 /// Note: If the preconditioner is a subsidiary
 /// preconditioner then only the sub-vectors associated with 
 /// the blocks of the subsidiary preconditioner will be included. Hence
 /// the length of v is master_nrow() whereas the total length of the s
 /// s vectors is Nrow.
 //============================================================================
 template<typename MATRIX> 
  void BlockPreconditioner<MATRIX>::get_block_vectors(const DoubleVector& v,
                                                      Vector<DoubleVector >& s)
  {
#ifdef PARANOID
   if (!v.distribution_pt()->setup())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the global vector v must be setup.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::get_block_vectors(...)",
                         OOMPH_EXCEPTION_LOCATION);    
    }
   if (*v.distribution_pt() != *this->master_distribution_pt())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the global vector v must match the "
                   << " specified master_distribution_pt(). \n"
                   << "i.e. Distribution_pt in the master preconditioner";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::get_block_vectors(...)",
                         OOMPH_EXCEPTION_LOCATION);  
    }
#endif

   // Number of block types
   const unsigned nblock = this->nblock_types();

   // if + only one processor
   //    + more than one processor but matrix_pt is not distributed
   // then use the serial get_block method
   if (Distribution_pt->communicator_pt()->nproc() == 1 ||
       !Distribution_pt->distributed())
    {

     // Vector of vectors for each section of residual vector
     s.resize(nblock);

     // pointer to the data in v
     double* v_pt = v.values_pt();

     // setup the block vector and then insert the data
     for (unsigned b = 0; b < nblock; b++)
      {
       s[b].rebuild(Block_distribution_pt[b]);
       double* s_pt = s[b].values_pt();
       unsigned nrow = s[b].nrow();
       for (unsigned i = 0; i < nrow; i++)
        {
         s_pt[i] = v_pt[this->Global_index[b][i]];
        }
      }
    }
   // otherwise use mpi
   else 
    {
#ifdef OOMPH_HAS_MPI
     // my rank
     unsigned my_rank = Distribution_pt->communicator_pt()->my_rank();

     // the number of processors
     unsigned nproc = Distribution_pt->communicator_pt()->nproc();

     // build the vectors
     s.resize(nblock);
     for (unsigned b = 0; b < nblock; b++)
      {
       s[b].rebuild(Block_distribution_pt[b]);
      }

     // determine the maximum number of rows to be sent or recv
     // and determine the number of blocks each processor will send and recv
     // communication for
     Vector<int> nblock_send(nproc,0);
     Vector<int> nblock_recv(nproc,0);
     unsigned max_n_send_or_recv = 0;
     for (unsigned p = 0; p < nproc; p++)
      {
       for (unsigned b = 0; b < nblock; b++)
        {
         max_n_send_or_recv = 
          std::max(max_n_send_or_recv,Nrows_to_send_for_get_block(b,p));
         max_n_send_or_recv = 
          std::max(max_n_send_or_recv,Nrows_to_recv_for_get_block(b,p));
         if (Nrows_to_send_for_get_block(b,p) > 0)
          {
           nblock_send[p]++;
          }
         if (Nrows_to_recv_for_get_block(b,p) > 0)
          {
           nblock_recv[p]++;
          }       
        }
      }

     // create a vectors of 1s the size of the nblock for the mpi indexed
     // data types
     int* block_lengths = new int[max_n_send_or_recv];
     for (unsigned i = 0; i < max_n_send_or_recv; i++)
      {
       block_lengths[i] = 1;
      }

     // perform the sends and receives
     Vector<MPI_Request> requests;
     for (unsigned p = 0; p < nproc; p++)
      {
       // send and recv with other processors
       if (p != my_rank)
        {
         // send
         if (nblock_send[p] > 0)
          {
           // create the datatypes vector
           MPI_Datatype block_send_types[nblock_send[p]];

           // create the datatypes
           unsigned ptr = 0;
           for (unsigned b = 0; b < nblock; b++)
            {
             if (Nrows_to_send_for_get_block(b,p) > 0)
              {
               MPI_Type_indexed(Nrows_to_send_for_get_block(b,p),block_lengths,
                                Rows_to_send_for_get_block(b,p),MPI_DOUBLE,
                                &block_send_types[ptr]);
               MPI_Type_commit(&block_send_types[ptr]);
               ptr++;
              }
            }

           // compute the displacements and lengths
           MPI_Aint displacements[nblock_send[p]];
           int lengths[nblock_send[p]];
           for (int i = 0; i < nblock_send[p]; i++)
            {
             lengths[i] = 1;
             displacements[i] = 0;
            }

           // build the final datatype
           MPI_Datatype type_send;
           MPI_Type_struct(nblock_send[p],lengths,displacements,
                           block_send_types,&type_send);
           MPI_Type_commit(&type_send);

           // send
           MPI_Request send_req;
           MPI_Isend(v.values_pt(),1,type_send,p,0,
                     Distribution_pt->communicator_pt()->mpi_comm(),&send_req);
           MPI_Type_free(&type_send);
           for (int i = 0; i < nblock_send[p]; i++)
            {
             MPI_Type_free(&block_send_types[i]);
            }
           requests.push_back(send_req);
          }

         // recv
         if (nblock_recv[p] > 0)
          {
           // create the datatypes vector
           MPI_Datatype block_recv_types[nblock_recv[p]];

           // and the displacements
           MPI_Aint displacements[nblock_recv[p]];

           // and the lengths
           int lengths[nblock_recv[p]];

           // all displacements are computed relative to s[0] values
           MPI_Aint displacements_base;
           MPI_Address(s[0].values_pt(),&displacements_base);

           // now build
           unsigned ptr = 0;
           for (unsigned b = 0; b < nblock; b++)
            {
             if (Nrows_to_recv_for_get_block(b,p) > 0)
              {
               MPI_Type_indexed(Nrows_to_recv_for_get_block(b,p),block_lengths,
                                Rows_to_recv_for_get_block(b,p),MPI_DOUBLE,
                                &block_recv_types[ptr]);
               MPI_Type_commit(&block_recv_types[ptr]);
               MPI_Address(s[b].values_pt(),&displacements[ptr]);
               displacements[ptr] -= displacements_base;
               lengths[ptr] = 1;
               ptr++;
              }
            }

           // build the final data type
           MPI_Datatype type_recv;
           MPI_Type_struct(nblock_recv[p],lengths,displacements,
                           block_recv_types,&type_recv);
           MPI_Type_commit(&type_recv);         

           // recv
           MPI_Request recv_req;
           MPI_Irecv(s[0].values_pt(),1,type_recv,p,0,
                     Distribution_pt->communicator_pt()->mpi_comm(),&recv_req);
           MPI_Type_free(&type_recv);
           for (int i = 0; i < nblock_recv[p]; i++)
            {
             MPI_Type_free(&block_recv_types[i]);
            }
           requests.push_back(recv_req);
          }
        }

       // communicate wih self
       else
        {
         const double* v_values_pt = v.values_pt();
         for (unsigned b = 0; b < nblock; b++)
          {
           double* w_values_pt = s[b].values_pt();
           for (unsigned i = 0; i < Nrows_to_send_for_get_block(b,p); i++)
            {
             w_values_pt[Rows_to_recv_for_get_block(b,p)[i]] = 
              v_values_pt[Rows_to_send_for_get_block(b,p)[i]];
            }
          }
        }
      }

     // and then just wait
     unsigned c = requests.size();
     Vector<MPI_Status> stat(c);
     if (c)
      {
       MPI_Waitall(c,&requests[0],&stat[0]);
      }
     delete[] block_lengths;

#else
     // throw error
     std::ostringstream error_message;
     error_message << "The preconditioner is distributed and on more than one "
                   << "processor. MPI is required.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner<MATRIX>::get_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);
#endif
    }
  }

 //============================================================================
 /// \short Takes the vector of block vectors, s, and copies its entries
 /// into the  naturally ordered vector, v. If this is a subsidiary
 /// block preconditioner only those entries in v that are 
 /// associated with its blocks are affected.
 //============================================================================
 template<typename MATRIX> 
  void BlockPreconditioner<MATRIX>::return_block_vectors
  (const Vector<DoubleVector >& s, DoubleVector& v)
  {
   // the number of blocks
   unsigned nblock = this->nblock_types();

#ifdef PARANOID
   if (!v.distribution_pt()->setup())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the global vector v must be setup.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::return_block_vectors(...)",
                         OOMPH_EXCEPTION_LOCATION);    
    }
   if (*v.distribution_pt() != *this->master_distribution_pt())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the global vector v must match the "
                   << " specified master_distribution_pt(). \n"
                   << "i.e. Distribution_pt in the master preconditioner";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::return_block_vectors(...)",
                         OOMPH_EXCEPTION_LOCATION);  
    }
   for (unsigned b = 0; b < nblock; b++)
    {
     if (!s[b].distribution_pt()->setup())
      {
       std::ostringstream error_message;
       error_message << "The distribution of the block vector " << b 
                     << " must be setup.";
       throw OomphLibError(error_message.str(),
                           "BlockPreconditioner::return_block_vectors(...)",
                           OOMPH_EXCEPTION_LOCATION);    
      }
     if (*s[b].distribution_pt() != *Block_distribution_pt[b])
      {
       std::ostringstream error_message;
       error_message << "The distribution of the block vector " << b 
                     << "must match the"
                     << " specified distribution at Block_distribution_pt["
                     << b << "]";
       throw OomphLibError(error_message.str(),
                           "BlockPreconditioner::return_block_vectors(...)",
                           OOMPH_EXCEPTION_LOCATION);  
      }
    }
#endif

   // if + only one processor
   //    + more than one processor but matrix_pt is not distributed
   // then use the serial get_block method
   if (Distribution_pt->communicator_pt()->nproc() == 1 ||
       !Distribution_pt->distributed())
    {
     double* v_pt = v.values_pt();
     for (unsigned b = 0; b < nblock; b++)
      {
       double* s_pt = s[b].values_pt();
       unsigned nrow = this->block_dimension(b);
       for (unsigned i = 0; i < nrow; i++)
        {
         v_pt[this->Global_index[b][i]] = s_pt[i];
        }
      }
    }
   // otherwise use mpi
   else 
    {
#ifdef OOMPH_HAS_MPI

     // my rank
     unsigned my_rank = Distribution_pt->communicator_pt()->my_rank();

     // the number of processors
     unsigned nproc = Distribution_pt->communicator_pt()->nproc();

     // determine the maximum number of rows to be sent or recv
     // and determine the number of blocks each processor will send and recv
     // communication for
     Vector<int> nblock_send(nproc,0);
     Vector<int> nblock_recv(nproc,0);
     unsigned max_n_send_or_recv = 0;
     for (unsigned p = 0; p < nproc; p++)
      {
       for (unsigned b = 0; b < nblock; b++)
        {
         max_n_send_or_recv = 
          std::max(max_n_send_or_recv,Nrows_to_send_for_get_block(b,p));
         max_n_send_or_recv = 
          std::max(max_n_send_or_recv,Nrows_to_recv_for_get_block(b,p));
         if (Nrows_to_send_for_get_block(b,p) > 0)
          {
           nblock_recv[p]++;
          }
         if (Nrows_to_recv_for_get_block(b,p) > 0)
          {
           nblock_send[p]++;
          }       
        }
      }

     // create a vectors of 1s the size of the nblock for the mpi indexed
     // data types
     int* block_lengths = new int[max_n_send_or_recv];
     for (unsigned i = 0; i < max_n_send_or_recv; i++)
      {
       block_lengths[i] = 1;
      }

     // perform the sends and receives
     Vector<MPI_Request> requests;
     for (unsigned p = 0; p < nproc; p++)
      {
       // send and recv with other processors
       if (p != my_rank)
        {
         // recv
         if (nblock_recv[p] > 0)
          {
           // create the datatypes vector
           MPI_Datatype block_recv_types[nblock_recv[p]];

           // create the datatypes
           unsigned ptr = 0;
           for (unsigned b = 0; b < nblock; b++)
            {
             if (Nrows_to_send_for_get_block(b,p) > 0)
              {
               MPI_Type_indexed(Nrows_to_send_for_get_block(b,p),block_lengths,
                                Rows_to_send_for_get_block(b,p),MPI_DOUBLE,
                                &block_recv_types[ptr]);
               MPI_Type_commit(&block_recv_types[ptr]);
               ptr++;
              }
            }

           // compute the displacements and lengths
           MPI_Aint displacements[nblock_recv[p]];
           int lengths[nblock_recv[p]];
           for (int i = 0; i < nblock_recv[p]; i++)
            {
             lengths[i] = 1;
             displacements[i] = 0;
            }

           // build the final datatype
           MPI_Datatype type_recv;
           MPI_Type_struct(nblock_recv[p],lengths,displacements,
                           block_recv_types,&type_recv);
           MPI_Type_commit(&type_recv);

           // recv
           MPI_Request recv_req;
           MPI_Irecv(v.values_pt(),1,type_recv,p,0,
                     Distribution_pt->communicator_pt()->mpi_comm(),&recv_req);
           MPI_Type_free(&type_recv);
           for (int i = 0; i < nblock_recv[p]; i++)
            {
             MPI_Type_free(&block_recv_types[i]);
            }
           requests.push_back(recv_req);
          }

         // send
         if (nblock_send[p] > 0)
          {
           // create the datatypes vector
           MPI_Datatype block_send_types[nblock_send[p]];

           // and the displacements
           MPI_Aint displacements[nblock_send[p]];

           // and the lengths
           int lengths[nblock_send[p]];

           // all displacements are computed relative to s[0] values
           MPI_Aint displacements_base;
           MPI_Address(s[0].values_pt(),&displacements_base);

           // now build
           unsigned ptr = 0;
           for (unsigned b = 0; b < nblock; b++)
            {
             if (Nrows_to_recv_for_get_block(b,p) > 0)
              {
               MPI_Type_indexed(Nrows_to_recv_for_get_block(b,p),block_lengths,
                                Rows_to_recv_for_get_block(b,p),MPI_DOUBLE,
                                &block_send_types[ptr]);
               MPI_Type_commit(&block_send_types[ptr]);
               MPI_Address(s[b].values_pt(),&displacements[ptr]);
               displacements[ptr] -= displacements_base;
               lengths[ptr] = 1;
               ptr++;
              }
            }

           // build the final data type
           MPI_Datatype type_send;
           MPI_Type_struct(nblock_send[p],lengths,displacements,
                           block_send_types,&type_send);
           MPI_Type_commit(&type_send);         

           // send
           MPI_Request send_req;
           MPI_Isend(s[0].values_pt(),1,type_send,p,0,
                     Distribution_pt->communicator_pt()->mpi_comm(),&send_req);
           MPI_Type_free(&type_send);
           for (int i = 0; i < nblock_send[p]; i++)
            {
             MPI_Type_free(&block_send_types[i]);
            }
           requests.push_back(send_req);
          }
        }

       // communicate wih self
       else
        {
         double* v_values_pt = v.values_pt();
         for (unsigned b = 0; b < nblock; b++)
          {
           const double* w_values_pt = s[b].values_pt();
           for (unsigned i = 0; i < Nrows_to_send_for_get_block(b,p); i++)
            {
             v_values_pt[Rows_to_send_for_get_block(b,p)[i]] =
              w_values_pt[Rows_to_recv_for_get_block(b,p)[i]]; 

            }
          }
        }
      }

     // and then just wait
     unsigned c = requests.size();
     Vector<MPI_Status> stat(c);
     if (c)
      {
       MPI_Waitall(c,&requests[0],&stat[0]);
      }
     delete[] block_lengths;

#else
     // throw error
     std::ostringstream error_message;
     error_message << "The preconditioner is distributed and on more than one "
                   << "processor. MPI is required.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner<MATRIX>::get_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);
#endif
    }
  }

 //============================================================================
 /// \short Takes the naturally ordered vector, v, and extracts
 /// the n-th block vector, b. Here n is the block number in the 
 /// current preconditioner.
 //============================================================================
 template<typename MATRIX> 
  void BlockPreconditioner<MATRIX>::get_block_vector(const unsigned& b,
                                                     const DoubleVector& v, 
                                                     DoubleVector& w)
  {
#ifdef PARANOID
   // the number of blocks
   unsigned n_blocks = this->nblock_types();

   // paranoid check that block i is in this block preconditioner
   if (b >= n_blocks)
    {
     std::ostringstream error_message;
     error_message << "Requested block  vector " << b  
                   << ", however this preconditioner has nblock_types() "
                   << "= " << nblock_types() << std::endl;
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::get_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);
    }
   if (!v.distribution_pt()->setup())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the global vector v must be setup.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::get_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);    
    }
   if (*v.distribution_pt() != *this->master_distribution_pt())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the global vector v must match the "
                   << " specified master_distribution_pt(). \n"
                   << "i.e. Distribution_pt in the master preconditioner";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::get_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);  
    }
#endif

   // rebuild the block vector
   w.rebuild(Block_distribution_pt[b]);

   // if + only one processor
   //    + more than one processor but matrix_pt is not distributed
   // then use the serial get_block method
   if (Distribution_pt->communicator_pt()->nproc() == 1 ||
       !Distribution_pt->distributed())
    {
     double* w_pt = w.values_pt();
     double* v_pt = v.values_pt();
     unsigned n_row = w.nrow();
     for (unsigned i = 0; i < n_row; i++)
      {
       w_pt[i] = v_pt[this->Global_index[b][i]];
      }
    }
   // otherwise use mpi
   else 
    {
#ifdef OOMPH_HAS_MPI

     // my rank
     unsigned my_rank = Distribution_pt->communicator_pt()->my_rank();

     // the number of processors
     unsigned nproc = Distribution_pt->communicator_pt()->nproc();

     // determine the maximum number of rows to be sent or recv
     unsigned max_n_send_or_recv = 0;
     for (unsigned p = 0; p < nproc; p++)
      {
       max_n_send_or_recv = 
        std::max(max_n_send_or_recv,Nrows_to_send_for_get_block(b,p));
       max_n_send_or_recv = 
        std::max(max_n_send_or_recv,Nrows_to_recv_for_get_block(b,p));
      }

     // create a vectors of 1s (the size of the nblock for the mpi indexed
     // data types
     int* block_lengths = new int[max_n_send_or_recv];
     for (unsigned i = 0; i < max_n_send_or_recv; i++)
      {
       block_lengths[i] = 1;
      }

     // perform the sends and receives
     Vector<MPI_Request> requests;
     for (unsigned p = 0; p < nproc; p++)
      {
       // send and recv with other processors
       if (p != my_rank)
        {
         if (Nrows_to_send_for_get_block(b,p) > 0)
          {
           // create the send datatype
           MPI_Datatype type_send;
           MPI_Type_indexed(Nrows_to_send_for_get_block(b,p),block_lengths,
                            Rows_to_send_for_get_block(b,p),MPI_DOUBLE,
                            &type_send);
           MPI_Type_commit(&type_send);

           // send
           MPI_Request send_req;
           MPI_Isend(v.values_pt(),1,type_send,p,0,
                     Distribution_pt->communicator_pt()->mpi_comm(),&send_req);
           MPI_Type_free(&type_send);
           requests.push_back(send_req);
          }

         if (Nrows_to_recv_for_get_block(b,p) > 0)
          {
           // create the recv datatype
           MPI_Datatype type_recv;
           MPI_Type_indexed(Nrows_to_recv_for_get_block(b,p),block_lengths,
                            Rows_to_recv_for_get_block(b,p),MPI_DOUBLE,
                            &type_recv);
           MPI_Type_commit(&type_recv);

           // recv
           MPI_Request recv_req;
           MPI_Irecv(w.values_pt(),1,type_recv,p,0,
                     Distribution_pt->communicator_pt()->mpi_comm(),&recv_req);
           MPI_Type_free(&type_recv);
           requests.push_back(recv_req);
          }
        }

       // communicate wih self
       else
        {
         double* w_values_pt = w.values_pt();
         const double* v_values_pt = v.values_pt();
         for (unsigned i = 0; i < Nrows_to_send_for_get_block(b,p); i++)
          {
           w_values_pt[Rows_to_recv_for_get_block(b,p)[i]] = 
            v_values_pt[Rows_to_send_for_get_block(b,p)[i]];
          }
        }
      }

     // and then just wait
     unsigned c = requests.size();
     Vector<MPI_Status> stat(c);
     if (c)
      {
       MPI_Waitall(c,&requests[0],&stat[0]);
      }
     delete[] block_lengths;

#else
     // throw error
     std::ostringstream error_message;
     error_message << "The preconditioner is distributed and on more than one "
                   << "processor. MPI is required.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner<MATRIX>::get_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);
#endif
    }
  }

 //============================================================================
 /// \short Takes the n-th block ordered vector, b,  and copies its entries
 /// to the appropriate entries in the naturally ordered vector, v.
 /// Here n is the block number in the current block preconditioner.
 /// If the preconditioner is a subsidiary block preconditioner
 /// the other entries in v  that are not associated with it
 /// are left alone
 //============================================================================
 template<typename MATRIX> 
  void BlockPreconditioner<MATRIX>::return_block_vector(const unsigned& b, 
                                                        const DoubleVector& w,
                                                        DoubleVector& v)
  {
#ifdef PARANOID
   // the number of blocks
   unsigned n_blocks = this->nblock_types();

   // paranoid check that block i is in this block preconditioner
   if (b >= n_blocks)
    {
     std::ostringstream error_message;
     error_message << "Requested block  vector " << b  
                   << ", however this preconditioner has nblock_types() "
                   << "= " << nblock_types() << std::endl;
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::return_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);
    }
   if (!v.distribution_pt()->setup())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the global vector v must be setup.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::return_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);    
    }
   if (*v.distribution_pt() != *this->master_distribution_pt())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the global vector v must match the "
                   << " specified master_distribution_pt(). \n"
                   << "i.e. Distribution_pt in the master preconditioner";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::return_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);  
    }
   if (!w.distribution_pt()->setup())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the block vector w must be setup.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::return_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);    
    }
   if (*w.distribution_pt() != *Block_distribution_pt[b])
    {
     std::ostringstream error_message;
     error_message << "The distribution of the block vector w must match the "
                   << " specified distribution at Block_distribution_pt[b]";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::return_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);  
    }
#endif

   // if + only one processor
   //    + more than one processor but matrix_pt is not distributed
   // then use the serial get_block method
   if (Distribution_pt->communicator_pt()->nproc() == 1 ||
       !Distribution_pt->distributed())
    {

     // length of vector
     unsigned n_row = this->block_dimension(b);

     // copy back from the block vector to the naturally ordered vector
     double* v_pt = v.values_pt();
     double* w_pt = w.values_pt();
     for (unsigned i = 0; i < n_row; i++)
      {
       v_pt[this->Global_index[b][i]] = w_pt[i];
      }
    }
   // otherwise use mpi
   else 
    {
#ifdef OOMPH_HAS_MPI

     // my rank
     unsigned my_rank = Distribution_pt->communicator_pt()->my_rank();

     // the number of processors
     unsigned nproc = Distribution_pt->communicator_pt()->nproc();

     // determine the maximum number of rows to be sent or recv
     unsigned max_n_send_or_recv = 0;
     for (unsigned p = 0; p < nproc; p++)
      {
       max_n_send_or_recv = 
        std::max(max_n_send_or_recv,Nrows_to_send_for_get_block(b,p));
       max_n_send_or_recv = 
        std::max(max_n_send_or_recv,Nrows_to_recv_for_get_block(b,p));
      }

     // create a vectors of 1s (the size of the nblock for the mpi indexed
     // data types
     int* block_lengths = new int[max_n_send_or_recv];
     for (unsigned i = 0; i < max_n_send_or_recv; i++)
      {
       block_lengths[i] = 1;
      }

     // perform the sends and receives
     Vector<MPI_Request> requests;
     for (unsigned p = 0; p < nproc; p++)
      {
       // send and recv with other processors
       if (p != my_rank)
        {
         if (Nrows_to_recv_for_get_block(b,p) > 0)
          {
           // create the send datatype
           MPI_Datatype type_send;
           MPI_Type_indexed(Nrows_to_recv_for_get_block(b,p),block_lengths,
                            Rows_to_recv_for_get_block(b,p),MPI_DOUBLE,
                            &type_send);
           MPI_Type_commit(&type_send);

           // send
           MPI_Request send_req;
           MPI_Isend(w.values_pt(),1,type_send,p,0,
                     Distribution_pt->communicator_pt()->mpi_comm(),&send_req);
           MPI_Type_free(&type_send);
           requests.push_back(send_req);
          }

         if (Nrows_to_send_for_get_block(b,p) > 0)
          {
           // create the recv datatype
           MPI_Datatype type_recv;
           MPI_Type_indexed(Nrows_to_send_for_get_block(b,p),block_lengths,
                            Rows_to_send_for_get_block(b,p),MPI_DOUBLE,
                            &type_recv);
           MPI_Type_commit(&type_recv);

           // recv
           MPI_Request recv_req;
           MPI_Irecv(v.values_pt(),1,type_recv,p,0,
                     Distribution_pt->communicator_pt()->mpi_comm(),&recv_req);
           MPI_Type_free(&type_recv);
           requests.push_back(recv_req);
          }
        }

       // communicate wih self
       else
        {
         const double* w_values_pt = w.values_pt();
         double* v_values_pt = v.values_pt();
         for (unsigned i = 0; i < Nrows_to_send_for_get_block(b,p); i++)
          {
           v_values_pt[Rows_to_send_for_get_block(b,p)[i]] =
            w_values_pt[Rows_to_recv_for_get_block(b,p)[i]];
          }
        }
      }

     // and then just wait
     unsigned c = requests.size();
     Vector<MPI_Status> stat(c);
     if (c)
      {
       MPI_Waitall(c,&requests[0],&stat[0]);
      }
     delete[] block_lengths;

#else
     // throw error
     std::ostringstream error_message;
     error_message << "The preconditioner is distributed and on more than one "
                   << "processor. MPI is required.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner<MATRIX>::get_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);
#endif
    }
  }

 //=============================================================================
 /// \short Given the naturally ordered vector, v, return
 /// the vector rearranged in block order in w.
 //=============================================================================
 template<typename MATRIX> 
  void BlockPreconditioner<MATRIX>::
  get_block_ordered_preconditioner_vector(const DoubleVector& v,
                                          DoubleVector& w)
  {
#ifdef PARANOID
   if (!v.distribution_pt()->setup())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the global vector v must be setup.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::get_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);    
    }
   if (*v.distribution_pt() != *this->master_distribution_pt())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the global vector v must match the "
                   << " specified master_distribution_pt(). \n"
                   << "i.e. Distribution_pt in the master preconditioner";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::get_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);  
    }
#endif

   //  Cleared and resized w for reordered vector
   w.rebuild(Distribution_pt);

   // if + only one processor
   //    + more than one processor but matrix_pt is not distributed
   // then use the serial get_block method
   if (Distribution_pt->communicator_pt()->nproc() == 1 ||
       !Distribution_pt->distributed())
    {

     // number of blocks
     unsigned nblock = this->Nblock_types;

     // copy to w
     unsigned block_offset = 0;
     double* w_pt = w.values_pt();
     double* v_pt = v.values_pt();
     for (unsigned b = 0; b < nblock;b++)
      {
       unsigned block_nrow = this->block_dimension(b);
       for (unsigned i = 0; i < block_nrow; i++)
        {
         w_pt[block_offset+i] = v_pt[this->Global_index[b][i]];
        }
       block_offset += block_nrow;
      }
    }
   // otherwise use mpi
   else 
    {
#ifdef OOMPH_HAS_MPI

     // my rank
     unsigned my_rank = Distribution_pt->communicator_pt()->my_rank();

     // the number of processors
     unsigned nproc = Distribution_pt->communicator_pt()->nproc();

     // determine the maximum number of rows to be sent or recv
     unsigned max_n_send_or_recv = 0;
     for (unsigned p = 0; p < nproc; p++)
      {
       max_n_send_or_recv = 
        std::max(max_n_send_or_recv,Nrows_to_send_for_get_ordered[p]);
       max_n_send_or_recv = 
        std::max(max_n_send_or_recv,Nrows_to_recv_for_get_ordered[p]);
      }

     // create a vectors of 1s (the size of the nblock for the mpi indexed
     // data types
     int* block_lengths = new int[max_n_send_or_recv];
     for (unsigned i = 0; i < max_n_send_or_recv; i++)
      {
       block_lengths[i] = 1;
      }

     // perform the sends and receives
     Vector<MPI_Request> requests;
     for (unsigned p = 0; p < nproc; p++)
      {
       // send and recv with other processors
       if (p != my_rank)
        {
         if (Nrows_to_send_for_get_ordered[p] > 0)
          {
           // create the send datatype
           MPI_Datatype type_send;
           MPI_Type_indexed(Nrows_to_send_for_get_ordered[p],block_lengths,
                            Rows_to_send_for_get_ordered[p],MPI_DOUBLE,
                            &type_send);
           MPI_Type_commit(&type_send);

           // send
           MPI_Request send_req;
           MPI_Isend(v.values_pt(),1,type_send,p,0,
                     Distribution_pt->communicator_pt()->mpi_comm(),&send_req);
           MPI_Type_free(&type_send);
           requests.push_back(send_req);
          }

         if (Nrows_to_recv_for_get_ordered[p] > 0)
          {
           // create the recv datatype
           MPI_Datatype type_recv;
           MPI_Type_indexed(Nrows_to_recv_for_get_ordered[p],block_lengths,
                            Rows_to_recv_for_get_ordered[p],MPI_DOUBLE,
                            &type_recv);
           MPI_Type_commit(&type_recv);

           // recv
           MPI_Request recv_req;
           MPI_Irecv(w.values_pt(),1,type_recv,p,0,
                     Distribution_pt->communicator_pt()->mpi_comm(),&recv_req);
           MPI_Type_free(&type_recv);
           requests.push_back(recv_req);
          }
        }

       // communicate wih self
       else
        {
         double* w_values_pt = w.values_pt();
         const double* v_values_pt = v.values_pt();
         for (unsigned i = 0; i < Nrows_to_send_for_get_ordered[p]; i++)
          {
           w_values_pt[Rows_to_recv_for_get_ordered[p][i]] = 
            v_values_pt[Rows_to_send_for_get_ordered[p][i]];
          }
        }
      }

     // and then just wait
     unsigned c = requests.size();
     Vector<MPI_Status> stat(c);
     if (c)
      {
       MPI_Waitall(c,&requests[0],&stat[0]);
      }
     delete[] block_lengths;

#else
     // throw error
     std::ostringstream error_message;
     error_message << "The preconditioner is distributed and on more than one "
                   << "processor. MPI is required.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner<MATRIX>::get_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);
#endif
    }
  }

 //============================================================================
 /// \short Takes the naturally ordered vector, w, and reorders it in 
 /// block order. Reordered vector is returned in v. Note: If the 
 /// preconditioner is a subsidiary
 /// preconditioner then only the components of the vector associated with 
 /// the blocks of the subsidiary preconditioner will be included. Hence
 /// the length of w is master_nrow() whereas that of the v is
 //============================================================================
 template<typename MATRIX> 
  void BlockPreconditioner<MATRIX>::
  return_block_ordered_preconditioner_vector(const DoubleVector& w, 
                                             DoubleVector& v)
  {
#ifdef PARANOID
   if (!v.distribution_pt()->setup())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the global vector v must be setup.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::return_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);    
    }
   if (*v.distribution_pt() != *this->master_distribution_pt())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the global vector v must match the "
                   << " specified master_distribution_pt(). \n"
                   << "i.e. Distribution_pt in the master preconditioner";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::return_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);  
    }
   if (!w.distribution_pt()->setup())
    {
     std::ostringstream error_message;
     error_message << "The distribution of the block vector w must be setup.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::return_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);    
    }
   if (*w.distribution_pt() != *Distribution_pt)
    {
     std::ostringstream error_message;
     error_message << "The distribution of the block vector w must match the "
                   << " specified distribution at Distribution_pt[b]";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner::return_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);  
    }
#endif


   // if + only one processor
   //    + more than one processor but matrix_pt is not distributed
   // then use the serial get_block method
   if (Distribution_pt->communicator_pt()->nproc() == 1 ||
       !Distribution_pt->distributed())
    {    
     // number of blocks
     unsigned nblock = this->Nblock_types;

     // copy to w
     unsigned block_offset = 0;
     double* w_pt = w.values_pt();
     double* v_pt = v.values_pt();
     for (unsigned b = 0; b < nblock;b++)
      {
       unsigned block_nrow = this->block_dimension(b);
       for (unsigned i = 0; i < block_nrow; i++)
        {
         v_pt[this->Global_index[b][i]] = w_pt[block_offset+i];
        }
       block_offset += block_nrow;
      }
    }
   // otherwise use mpi
   else 
    {
#ifdef OOMPH_HAS_MPI

     // my rank
     unsigned my_rank = Distribution_pt->communicator_pt()->my_rank();

     // the number of processors
     unsigned nproc = Distribution_pt->communicator_pt()->nproc();

     // determine the maximum number of rows to be sent or recv
     unsigned max_n_send_or_recv = 0;
     for (unsigned p = 0; p < nproc; p++)
      {
       max_n_send_or_recv = 
        std::max(max_n_send_or_recv,Nrows_to_send_for_get_ordered[p]);
       max_n_send_or_recv = 
        std::max(max_n_send_or_recv,Nrows_to_recv_for_get_ordered[p]);
      }

     // create a vectors of 1s (the size of the nblock for the mpi indexed
     // data types
     int* block_lengths = new int[max_n_send_or_recv];
     for (unsigned i = 0; i < max_n_send_or_recv; i++)
      {
       block_lengths[i] = 1;
      }

     // perform the sends and receives
     Vector<MPI_Request> requests;
     for (unsigned p = 0; p < nproc; p++)
      {
       // send and recv with other processors
       if (p != my_rank)
        {
         if (Nrows_to_recv_for_get_ordered[p] > 0)
          {
           // create the send datatype
           MPI_Datatype type_send;
           MPI_Type_indexed(Nrows_to_recv_for_get_ordered[p],block_lengths,
                            Rows_to_recv_for_get_ordered[p],MPI_DOUBLE,
                            &type_send);
           MPI_Type_commit(&type_send);

           // send
           MPI_Request send_req;
           MPI_Isend(w.values_pt(),1,type_send,p,0,
                     Distribution_pt->communicator_pt()->mpi_comm(),&send_req);
           MPI_Type_free(&type_send);
           requests.push_back(send_req);
          }

         if (Nrows_to_send_for_get_ordered[p] > 0)
          {
           // create the recv datatype
           MPI_Datatype type_recv;
           MPI_Type_indexed(Nrows_to_send_for_get_ordered[p],block_lengths,
                            Rows_to_send_for_get_ordered[p],MPI_DOUBLE,
                            &type_recv);
           MPI_Type_commit(&type_recv);

           // recv
           MPI_Request recv_req;
           MPI_Irecv(v.values_pt(),1,type_recv,p,0,
                     Distribution_pt->communicator_pt()->mpi_comm(),&recv_req);
           MPI_Type_free(&type_recv);
           requests.push_back(recv_req);
          }
        }

       // communicate wih self
       else
        {
         const double* w_values_pt = w.values_pt();
         double* v_values_pt = v.values_pt();
         for (unsigned i = 0; i < Nrows_to_send_for_get_ordered[p]; i++)
          {
           v_values_pt[Rows_to_send_for_get_ordered[p][i]] =
            w_values_pt[Rows_to_recv_for_get_ordered[p][i]];
          }
        }
      }

     // and then just wait
     unsigned c = requests.size();
     Vector<MPI_Status> stat(c);
     if (c)
      {
       MPI_Waitall(c,&requests[0],&stat[0]);
      }
     delete[] block_lengths;

#else
     // throw error
     std::ostringstream error_message;
     error_message << "The preconditioner is distributed and on more than one "
                   << "processor. MPI is required.";
     throw OomphLibError(error_message.str(),
                         "BlockPreconditioner<MATRIX>::get_block_vector(...)",
                         OOMPH_EXCEPTION_LOCATION);
#endif
    }
  }
}
#endif
