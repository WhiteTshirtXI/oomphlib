//LIC// ====================================================================
//LIC// This file forms part of oomph-lib, the object-oriented, 
//LIC// multi-physics finite-element library, available 
//LIC// at http://www.oomph-lib.org.
//LIC// 
//LIC//           Version 0.85. June 9, 2008.
//LIC// 
//LIC// Copyright (C) 2006-2008 Matthias Heil and Andrew Hazel
//LIC// 
//LIC// This library is free software; you can redistribute it and/or
//LIC// modify it under the terms of the GNU Lesser General Public
//LIC// License as published by the Free Software Foundation; either
//LIC// version 2.1 of the License, or (at your option) any later version.
//LIC// 
//LIC// This library is distributed in the hope that it will be useful,
//LIC// but WITHOUT ANY WARRANTY; without even the implied warranty of
//LIC// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
//LIC// Lesser General Public License for more details.
//LIC// 
//LIC// You should have received a copy of the GNU Lesser General Public
//LIC// License along with this library; if not, write to the Free Software
//LIC// Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
//LIC// 02110-1301  USA.
//LIC// 
//LIC// The authors may be contacted at oomph-lib@maths.man.ac.uk.
//LIC// 
//LIC//====================================================================
#ifndef OOMPH_DISTRIBUTION_INFO_CLASS_HEADER
#define OOMPH_DISTRIBUTION_INFO_CLASS_HEADER

// Config header generated by autoconfig
#ifdef HAVE_CONFIG_H
  #include <oomph-lib-config.h>
#endif

//only build if oomph has mpi
#ifdef OOMPH_HAS_MPI

// General headers
#include "../generic/Vector.h"

// MPI headers
#include "mpi.h"

namespace oomph{



//=============================================================================
/// \short describes the distribution of a distributed object. We assume that 
/// each processor contains a single continuous block of rows - parametised
/// with variables denoting the first row and the number of local rows. This
/// class contains the member data to store this information and the functions 
/// to access and edit this data. The MPI_Comm object Communicator is the 
/// communicator associated with this distribution - it should be used for
/// communication of objects of this distribution.
//=============================================================================
class DistributionInfo
{

 public :

  /// \short Constructor. Takes the first_row, nrow_local and nrow_global as
  /// arguments. If nrow_global is not provided or equal to 0 then it
  /// will be computed automatically
  DistributionInfo(MPI_Comm comm,
                   const long unsigned& first_row, 
                   const long unsigned& nrow_local,
                   const long unsigned& nrow_global = 0)
  {
   distribute(comm,first_row,nrow_local,nrow_global);
  };
 
 /// \short Constructor. Takes the number of global rows and uniformly 
 /// distributes them over the processors
 DistributionInfo(MPI_Comm comm, 
                  const long unsigned& nrow_global)
  {
   distribute(comm, nrow_global);
  };
 
 /// Constructor. For a distribution that has not been setup
 DistributionInfo() : Setup(false)
  {};
 
 /// \short Destructor 
 ~DistributionInfo() 
  {
  }
 
 /// \short Copy Constructor.
 DistributionInfo(const DistributionInfo& dist)          
  {
   if (dist.setup())
    {
     distribute(dist.communicator(),dist.first_row(),dist.nrow_local(),
                dist.nrow_global());
    }
   else
    {
     Setup = false;
    }
  }
 
 /// Assignment Operator
 void operator=(const DistributionInfo& dist)  
  {                                                                        
   if (dist.setup()) 
    {  
     distribute(dist.communicator(),
                dist.first_row(),dist.nrow_local(),dist.nrow_global());
    }    
   else   
    { 
     Setup = false;
    }       
  }
 
 /// \short Sets the distribution. Takes first_row, nrow_local and 
 /// nrow_global as arguments. If nrow_global is not provided or equal to
 /// 0 then it is computed automatically
 void distribute(MPI_Comm comm,
                 const long unsigned& first_row, 
                 const long unsigned& nrow_local,
                 const long unsigned& nrow_global = 0);

 /// \short Uniformly distribute nrow_global over all processors where 
 /// processors 0 holds approximately the first 
 /// nrow_global/n_proc, processor 1 holds the next 
 /// nrow_global/n_proc and so on...
 void distribute(MPI_Comm comm,
                 const long unsigned& nrow_global);
 
 /// \short clears the distribution \n
 /// \b distribution is not setup
 /// \b all member data = 0
 /// \b null communicator
 void clear()
  {
   Communicator = MPI_COMM_NULL;
   First_row.clear();
   Nrow_local.clear();
   Nrow_global = 0;
   Setup = false;
  }
 
 /// \short returns the distribution on all processors in a pair of vectors 
 /// where the i-th index corresponds to the distribution of the i-th processor
 void full_distribution(MPI_Comm& comm,
                        Vector<long unsigned>& dist_first_row,
                        Vector<long unsigned>& dist_nrow_local)
  {
#ifdef PARANOID
   if (!Setup)
    { 
     throw OomphLibError("Distribution has not been set.",
                         "DistributionInfo::full_distribution()",     
                         OOMPH_EXCEPTION_LOCATION);
    }                                                           
#endif        
   comm = Communicator;
   dist_first_row = First_row;
   dist_nrow_local = Nrow_local;
  }
 
 /// access fn to num of global rows (const)
 unsigned long nrow_global() const
  {
#ifdef PARANOID
   if (!Setup)
    {                                                                        
     throw OomphLibError("Distribution has not been set.",
                         "DistributionInfo::nrow_global()",
                         OOMPH_EXCEPTION_LOCATION);   
    }
#endif
   return Nrow_global; 
  }

 /// \short access function for the num of local rows on this processor
 unsigned long nrow_local() const
  {
#ifdef PARANOID
   if (!Setup)
    { 
     throw OomphLibError("Distribution has not been set.",
                         "DistributionInfo::nrow_local()",     
                         OOMPH_EXCEPTION_LOCATION);
    }                                                           
#endif   

   // find out the rank of this processor
   int my_rank;
   MPI_Comm_rank(Communicator,&my_rank);

   // return the first row
   return nrow_local(my_rank);
  }
 
 /// \short access fn to num of local rows on processor p. If no arguments 
 /// then number of local rows on this processor returned. (const)
 unsigned long nrow_local(const unsigned& p) const
  { 
#ifdef PARANOID
   if (!Setup)
    { 
     throw OomphLibError("Distribution has not been set.",
                         "DistributionInfo::nrow_local()",     
                         OOMPH_EXCEPTION_LOCATION);
    }                                                           
#endif           
    return Nrow_local[p]; 
   }
 
 /// \short access function for the first row on this processor
 unsigned long first_row() const
  {
#ifdef PARANOID
   if (!Setup)
    {
     throw OomphLibError("Distribution has not been set.",
                         "DistributionInfo::first_row()",     
                         OOMPH_EXCEPTION_LOCATION);
    }
#endif
   // find out the rank of this processor
   int my_rank;
   MPI_Comm_rank(Communicator,&my_rank);

   // return the first row
   return first_row(my_rank);
  }

 /// \short access fn to the first row for processor p.
 unsigned long first_row(const unsigned& p) const
  { 
#ifdef PARANOID
   if (!Setup)
    {
     throw OomphLibError("Distribution has not been set.",
                         "DistributionInfo::first_row()",      
                         OOMPH_EXCEPTION_LOCATION);
     }   
#endif           
   return First_row[p]; 
  }
 
 /// access fn to find out whether this distribution is setup
 bool setup() const { return Setup; }
 
 // \short == Operator
 bool operator==(DistributionInfo dist)
  {   

   // if both dist are not setup the return true
   if ((dist.setup() == false) && (Setup == false))
    {
     return true;
    }

   // if one dist is setup and the other isn't
   if (((dist.setup() == true) && (Setup == false)) ||
       ((dist.setup() == false) && (Setup == true)))
    {
     return false;
    }

   // get the full distribution
   Vector <long unsigned> dist_first_row;
   Vector <long unsigned> dist_nrow_local;
   MPI_Comm dist_comm;
   dist.full_distribution(dist_comm,dist_first_row,dist_nrow_local);
   
   // compare
   if ((dist_first_row == First_row) &&
       (dist_nrow_local == Nrow_local))
    {
     return false;
    }
   else
    {
     return true;
    }
  }
 
 // \short != Operator
 bool operator!=(DistributionInfo dist)
  {
   // if both dist are not setup then return false
   if ((dist.setup() == false) && (Setup == false))       
    {
     return false;
    }

   // if one dist is setup and the other isn't
   if (((dist.setup() == true) && (Setup == false)) ||
       ((dist.setup() == false) && (Setup == true)))
    {
     return true;
    }

   // get the full distribution
   Vector <long unsigned> dist_first_row;
   Vector <long unsigned> dist_nrow_local;
   MPI_Comm dist_comm;
   dist.full_distribution(dist_comm,dist_first_row,dist_nrow_local);

   // compare
   if ((dist_first_row == First_row) &&
       (dist_nrow_local == Nrow_local))
    {
     return false;
    }
   else
    {
     return true;
    }
  }

 /// access function to the communicator
 MPI_Comm communicator() { return Communicator; }

 /// access function to the communicator (const version)
 MPI_Comm communicator() const { return Communicator; }

  private:
 
 /// the number of global rows
 unsigned long Nrow_global;
 
 /// the number of local rows on the processor
 Vector<unsigned long> Nrow_local;
 
 /// the first row on this processor
 Vector<unsigned long> First_row;
 
 /// boolean indicating whether the distribution has been setup
 bool Setup;
 
 /// the pointer to the MPI communicator object in this distribution
 MPI_Comm Communicator;

}; //end of DistributionInfo



} // end of oomph namespace
#endif // OOMPH_HAS_MPI endif
#endif
